[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "course-notes/index.html",
    "href": "course-notes/index.html",
    "title": "Course notes",
    "section": "",
    "text": "First lecture\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nprint('Hello')\n\nHello"
  },
  {
    "objectID": "course-notes/econometrics-ml/first-lecture/index.html",
    "href": "course-notes/econometrics-ml/first-lecture/index.html",
    "title": "First lecture",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability Calibration & Cost classification models\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html",
    "href": "posts/probability-calibration/prob_calibration.html",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "We can say that a well calibrated classifier is one that its probabilities corresponds to the real fraction of positives we observe in real life, meaning that we could interpret its probabilities as a confidence level as well. An example of a well calibrated (binary) classifier is one that if in 100 examples it predicts a probability of 0.7, then 70% of the examples will have class 1 and 30% will have a class 0.\n\n\n\nIf we have a problem that we only care about the predictions of class 1 or 0, then we don’t need to care for calibration, but if it is the case that the probabilities of the model will be used in our real life situation, then we would want that the probabilities really represents the likelihood of these events.\nLet’s use an example of fraud detection, we can train a ML model to predict if a transaction is fraudulent or not (1/0). But for fraud detection, it is often the case that we need to analyze the costs involved in the prediction to determine if a fraud is likely fraudulent or not. A framework that can be used is the following:\n\nSuppose we get a 10% comission over the amount transactioned if the transaction is complete, on the other hand, if the transacition was actually a fraud, then we would have to return to the customer 100% of the amount paid. In this case, a False Negative (FN) means that we would have a cost of -total amount, and a True Negative, a profit of 10% * total amount\nIn this scenario, there are different costs associated with each prediction, so we could classify the examples based on the expected revenue for class 1, and the expected revenue for class 0. In the end, we can choose the prediction that has the highest expected revenue:\n\\[\\textnormal{expected revenue 1}  = (profitTP * prob1) + (costFP * prob0)\\] \\[\\textnormal{expected revenue 0} = (profitTN * prob0) + (costFN * prob1)\\]\nFor each value in the confusion matrix, we can assign the following profit/cost structure:\n\nTP: 0 cost or profit because we classify the fraudulent transaction as fraud\nTN: 10% comission over the total amount\nFP: -10% comission, beacause we classify a fraud as one when it isn’t\nFN: -total amount, real fraud that wasn’t captured by the model -&gt; we have to return the total amount to the customer\n\nLet’s use as an example a transaction of $50 that is not fraudulent and assume our model predicts a probability of 0.4 of being fraudulent, when the real probability was 0.1. Here is the impact of this difference:\n\n\ndef get_example_value_dependent(prob_1, total_amount): \n    prob_0 = 1 - prob_1\n\n    rev_tp = 0\n    rev_fp = - (total_amount * 0.1)\n    rev_tn = total_amount * 0.1\n    rev_fn = -total_amount\n\n    expected_rev_1 =  (rev_tp * prob_1) + (rev_fp * prob_0)\n    expected_rev_0 =  (rev_tn * prob_0) + (rev_fn * prob_1)\n\n    return expected_rev_1, expected_rev_0\n\ndef classify_sample(expected_rev_1, expected_rev_0): \n    if expected_rev_1 &gt; expected_rev_0: return 1\n    else: return 0\n\n\nmodel_probability = 0.4\nexpected_rev_1, expected_rev_0 = get_example_value_dependent(model_probability, total_amount = 50)\nprint(f'Expected revenue class 1: {expected_rev_1}')\nprint(f'Expected revenue class 0: {expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample(expected_rev_1, expected_rev_0)}\\n')\n\nreal_probability = 0.1 \nreal_expected_rev_1, real_expected_rev_0 = get_example_value_dependent(real_probability, total_amount = 50)\nprint(f'Real expected revenue class 1: {real_expected_rev_1}')\nprint(f'Real expected revenue class 0: {real_expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample(real_expected_rev_1, real_expected_rev_0)}')\n\nExpected revenue class 1: -3.0\nExpected revenue class 0: -17.0\nClassification by model probability: 1\n\nReal expected revenue class 1: -4.5\nReal expected revenue class 0: -0.5\nClassification by model probability: 0\n\n\n\n\n\n\nAlgorithms not trained using a probabilistic framework\nThere are only a few models that produce calibrated probabilities. That is because in order to give calibrated probabilities, the model must be trained in a probabilistic framework, such as maximum likelihood estimation. The main example that can return probabilities already calibrated is a Logistic Regression. ANNs can also have better calibration of probabilities than other models.\nThe opposite can occur for methods such as in Bagging Estimators. For a Random Forest to predict a probability = 0, it means that all estimators in the forest predict 0. In a Random Forest this is harder to occur because of the feature subsample used to build each tree, meaning that there is a relative high variance between the trees in the forest.\nClass imbalance\nWhen there is a case of high class imbalance, such as in fraud prevention problems, the model will naturally predict higher probabilities for the majority class.\n\n\n\n\nFor the rest of this post, we’ll use the Credit Card Fraud Detection dataset available on Kaggle. Our goal will be to train a model to predict fraud and then use the same framework as before to classify each example as a fraud or not considering the expected profits and costs with each classification\n\ndf = pd.read_csv('creditcard.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n0.090794\n-0.551600\n-0.617801\n-0.991390\n-0.311169\n1.468177\n-0.470401\n0.207971\n0.025791\n0.403993\n0.251412\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n-0.166974\n1.612727\n1.065235\n0.489095\n-0.143772\n0.635558\n0.463917\n-0.114805\n-0.183361\n-0.145783\n-0.069083\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n0.207643\n0.624501\n0.066084\n0.717293\n-0.165946\n2.345865\n-2.890083\n1.109969\n-0.121359\n-2.261857\n0.524980\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n-0.054952\n-0.226487\n0.178228\n0.507757\n-0.287924\n-0.631418\n-1.059647\n-0.684093\n1.965775\n-1.232622\n-0.208038\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n0.753074\n-0.822843\n0.538196\n1.345852\n-1.119670\n0.175121\n-0.451449\n-0.237033\n-0.038195\n0.803487\n0.408542\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n\n\n\n\ndf.Class.value_counts(normalize = False)\n\n0    284315\n1       492\nName: Class, dtype: int64\n\n\n\nclass_0 = df.query('Class == 0').sample(200_000)\nclass_1 = df.query('Class == 1')\n\nfraud_data = pd.concat([class_0, class_1], axis = 0).reset_index(drop = True)\n\n\nfraud_data.Class.value_counts(normalize = True)\n\n0    0.997546\n1    0.002454\nName: Class, dtype: float64\n\n\n\nX = fraud_data.drop(['Class'], axis = 1)\ny = fraud_data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()\n\n\n\nmodel_preds = model.predict(X_test)\n\nprint(classification_report(y_test, model_preds))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     50001\n           1       0.96      0.73      0.83       122\n\n    accuracy                           1.00     50123\n   macro avg       0.98      0.86      0.91     50123\nweighted avg       1.00      1.00      1.00     50123\n\n\n\n\n\n\n\n\nTo better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn’s calibration module, this can be easily done:\n\nfrom sklearn.calibration import calibration_curve, CalibrationDisplay\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Plot calibration curve \nprob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=5)\ndisp = CalibrationDisplay(prob_true, prob_pred, model_probs)\ndisp.plot();\n\n\n\n\nLet’s also train a few more models so we can compare them:\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nlgbm = LGBMClassifier(verbose = -1)\nlgbm.fit(X_train, y_train)\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n[LightGBM] [Info] Number of positive: 370, number of negative: 149999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 150369, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004881\n[LightGBM] [Info] Start training from score -6.004881\n\n\n\n\n\nTO DO - add Brier score loss\n\n\n\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities.\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses a cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, and then the process is repeated over k times.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5)\n    calibrated_clf.fit(X_train, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004889 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004934 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 120000\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120296, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004888\n[LightGBM] [Info] Start training from score -6.004888\n\n\nTO DO\nfazer gráfico comparando antes e depois\n\n\n\n\nNow that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_rf = calibrated_clfs['Random forest'].predict_proba(X_test)[:, 1], \n    probs_lr = calibrated_clfs['Logistic Regression'].predict_proba(X_test)[:, 1],\n    y_true = y_test\n)\n\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_rf', 'probs_lr']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n\n    # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n        expected_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.expected_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.expected_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  437325.17\n\n probs_nb expected revenue:  375923.19\n\n probs_rf expected revenue:  438773.51\n\n probs_lr expected revenue:  435721.11\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\ndf_test_models_uncalib = X_test.assign(\n    probs_lgbm = lgbm.predict_proba(X_test)[:, 1], \n    probs_nb = nb.predict_proba(X_test)[:, 1], \n    probs_rf = model.predict_proba(X_test)[:, 1], \n    probs_lr = lr.predict_proba(X_test)[:, 1],\n    y_true = y_test\n)\n\n\nrevenues = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_rf', 'probs_lr']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_uncalib[p], df_test_models_uncalib['Amount'])\n\n    # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n    df_rev_uncalib = df_test_models_uncalib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n        expected_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues[p] = round(df_rev_uncalib.expected_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev_uncalib.expected_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  433334.93\n\n probs_nb expected revenue:  359968.59\n\n probs_rf expected revenue:  437874.19\n\n probs_lr expected revenue:  434726.94\n\n\n\ncompare_revs = pd.concat([pd.DataFrame(revenues_calibrated, index = ['revenues_calibrated']).T, \n                        pd.DataFrame(revenues, index = ['revenues_not_calibrated']).T], axis = 1\n                    ).assign(\n                        revenue_diff = lambda d: d['revenues_calibrated'] - d['revenues_not_calibrated']\n                    )\n\ncompare_revs\n\n\n\n\n\n\n\n\nrevenues_calibrated\nrevenues_not_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n437325.17\n433334.93\n3990.24\n\n\nprobs_nb\n375923.19\n359968.59\n15954.60\n\n\nprobs_rf\n438773.51\n437874.19\n899.32\n\n\nprobs_lr\n435721.11\n434726.94\n994.17\n\n\n\n\n\n\n\n\nbest_model = compare_revs.revenues_calibrated.idxmax()\n\nprint(f'Best model probabilities are {best_model} with a total revenue of {compare_revs.loc[best_model, \"revenues_calibrated\"]}')\n\nBest model probabilities are probs_rf with a total revenue of 438773.51"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "href": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "To better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn’s calibration module, this can be easily done:\n\nfrom sklearn.calibration import calibration_curve, CalibrationDisplay\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Plot calibration curve \nprob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=5)\ndisp = CalibrationDisplay(prob_true, prob_pred, model_probs)\ndisp.plot();\n\n\n\n\nLet’s also train a few more models so we can compare them:\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nlgbm = LGBMClassifier(verbose = -1)\nlgbm.fit(X_train, y_train)\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n[LightGBM] [Info] Number of positive: 370, number of negative: 149999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 150369, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004881\n[LightGBM] [Info] Start training from score -6.004881\n\n\n\n\n\nTO DO - add Brier score loss\n\n\n\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities.\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses a cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, and then the process is repeated over k times.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5)\n    calibrated_clf.fit(X_train, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004889 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004934 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 119999\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120295, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004879\n[LightGBM] [Info] Start training from score -6.004879\n[LightGBM] [Info] Number of positive: 296, number of negative: 120000\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7650\n[LightGBM] [Info] Number of data points in the train set: 120296, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002461 -&gt; initscore=-6.004888\n[LightGBM] [Info] Start training from score -6.004888\n\n\nTO DO\nfazer gráfico comparando antes e depois"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "href": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "Now that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_rf = calibrated_clfs['Random forest'].predict_proba(X_test)[:, 1], \n    probs_lr = calibrated_clfs['Logistic Regression'].predict_proba(X_test)[:, 1],\n    y_true = y_test\n)\n\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_rf', 'probs_lr']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n\n    # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n        expected_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.expected_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.expected_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  437325.17\n\n probs_nb expected revenue:  375923.19\n\n probs_rf expected revenue:  438773.51\n\n probs_lr expected revenue:  435721.11\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\ndf_test_models_uncalib = X_test.assign(\n    probs_lgbm = lgbm.predict_proba(X_test)[:, 1], \n    probs_nb = nb.predict_proba(X_test)[:, 1], \n    probs_rf = model.predict_proba(X_test)[:, 1], \n    probs_lr = lr.predict_proba(X_test)[:, 1],\n    y_true = y_test\n)\n\n\nrevenues = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_rf', 'probs_lr']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_uncalib[p], df_test_models_uncalib['Amount'])\n\n    # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n    df_rev_uncalib = df_test_models_uncalib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n        expected_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues[p] = round(df_rev_uncalib.expected_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev_uncalib.expected_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  433334.93\n\n probs_nb expected revenue:  359968.59\n\n probs_rf expected revenue:  437874.19\n\n probs_lr expected revenue:  434726.94\n\n\n\ncompare_revs = pd.concat([pd.DataFrame(revenues_calibrated, index = ['revenues_calibrated']).T, \n                        pd.DataFrame(revenues, index = ['revenues_not_calibrated']).T], axis = 1\n                    ).assign(\n                        revenue_diff = lambda d: d['revenues_calibrated'] - d['revenues_not_calibrated']\n                    )\n\ncompare_revs\n\n\n\n\n\n\n\n\nrevenues_calibrated\nrevenues_not_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n437325.17\n433334.93\n3990.24\n\n\nprobs_nb\n375923.19\n359968.59\n15954.60\n\n\nprobs_rf\n438773.51\n437874.19\n899.32\n\n\nprobs_lr\n435721.11\n434726.94\n994.17\n\n\n\n\n\n\n\n\nbest_model = compare_revs.revenues_calibrated.idxmax()\n\nprint(f'Best model probabilities are {best_model} with a total revenue of {compare_revs.loc[best_model, \"revenues_calibrated\"]}')\n\nBest model probabilities are probs_rf with a total revenue of 438773.51"
  }
]