[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, I’m Julia! I currently work as a Data Scientist at Mercado Libre.\nMy journey into the world of Data Science began during my college years, and it was during this time that I became a member of Turing USP. This is a student-led group at the University of São Paulo, dedicated to the realm of Artificial Intelligence. When I initially joined the group, I had little to no knowledge about AI, but we had a system that knowing the subjects first hand or not really didn’t matter. The goal was to pick a topic that you found interesting, study it, and then share with the rest of the group. That way of learning really helped me, and to this day I believe that is the best way to learn something new.\nWith that in mind, my idea with this blog is to make it as a way for me to better study topics I’m interested in, share notes on courses I’m taking, and also use it in general as a way to better structure thoughts and experiments I’m currently working on."
  },
  {
    "objectID": "course-notes/index.html",
    "href": "course-notes/index.html",
    "title": "Course notes",
    "section": "",
    "text": "First lecture\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html",
    "href": "posts/everyday-pandas/everyday_pandas.html",
    "title": "Everyday pandas",
    "section": "",
    "text": "Pandas is probably the most used python library by any Data Scientist. It is the library that we use it everyday at work and import pandas as pd is usually the first thing we type into a notebook. Even though it’s such a common tool, I feel like I’m always learning about new functions and ways to explore data in pandas that I didn’t know of, specially looking back at how I used it when I first started interning and how I use it now.\nI thought about writing this as a survey on ways to explore data with pandas, with some of the things I learned over the years. Most of it was just things I learned looking at other people’s code and with stackoverflow.\nimport pandas as pd\nFor this post, I am going to use a dataset on matches played on World Cups. Here is a sample of the data:\ndf.head()\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\nattendance\nhalf-time_home_goals\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\n\n\n\n\n0\n1930\n13 Jul 1930 - 15:00\nGroup 1\nPocitos\nMontevideo\nFrance\n4.0\n1.0\nMexico\n\n4444.0\n3.0\n0.0\nLOMBARDI Domingo (URU)\nCRISTOPHE Henry (BEL)\nREGO Gilberto (BRA)\n201.0\n1096.0\nFRA\nMEX\n\n\n1\n1930\n13 Jul 1930 - 15:00\nGroup 4\nParque Central\nMontevideo\nUSA\n3.0\n0.0\nBelgium\n\n18346.0\n2.0\n0.0\nMACIAS Jose (ARG)\nMATEUCCI Francisco (URU)\nWARNKEN Alberto (CHI)\n201.0\n1090.0\nUSA\nBEL\n\n\n2\n1930\n14 Jul 1930 - 12:45\nGroup 2\nParque Central\nMontevideo\nYugoslavia\n2.0\n1.0\nBrazil\n\n24059.0\n2.0\n0.0\nTEJADA Anibal (URU)\nVALLARINO Ricardo (URU)\nBALWAY Thomas (FRA)\n201.0\n1093.0\nYUG\nBRA\n\n\n3\n1930\n14 Jul 1930 - 14:50\nGroup 3\nPocitos\nMontevideo\nRomania\n3.0\n1.0\nPeru\n\n2549.0\n1.0\n0.0\nWARNKEN Alberto (CHI)\nLANGENUS Jean (BEL)\nMATEUCCI Francisco (URU)\n201.0\n1098.0\nROU\nPER\n\n\n4\n1930\n15 Jul 1930 - 16:00\nGroup 1\nParque Central\nMontevideo\nArgentina\n1.0\n0.0\nFrance\n\n23409.0\n0.0\n0.0\nREGO Gilberto (BRA)\nSAUCEDO Ulises (BOL)\nRADULESCU Constantin (ROU)\n201.0\n1085.0\nARG\nFRA"
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#query",
    "href": "posts/everyday-pandas/everyday_pandas.html#query",
    "title": "Everyday pandas",
    "section": "query",
    "text": "query\nquery is really just a way to specify how you want to filter the dataset. If you want to select the games played by Brazil as a home team and that Brazil scored 5 goals or more, you can achieve this with the following code:\n\n# Select only BRA plays as home team and when it scored 5 goals or more \ndf.query('home_team_initials == \"BRA\" & home_team_goals &gt;= 5')\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\nattendance\nhalf-time_home_goals\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\n\n\n\n\n40\n1938\n05 Jun 1938 - 17:30\nFirst round\nStade de la Meinau\nStrasbourg\nBrazil\n6.0\n5.0\nPoland\nBrazil win after extra time\n13452.0\n0.0\n0.0\nEKLIND Ivan (SWE)\nPOISSANT Louis (FRA)\nKISSENBERGER Ernest (FRA)\n206.0\n1150.0\nBRA\nPOL\n\n\n70\n1950\n09 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n7.0\n1.0\nSweden\n\n138886.0\n3.0\n0.0\nELLIS Arthur (ENG)\nGARCIA Prudencio (USA)\nDE LA SALLE Charles (FRA)\n209.0\n1189.0\nBRA\nSWE\n\n\n71\n1950\n13 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n6.0\n1.0\nSpain\n\n152772.0\n3.0\n0.0\nLEAFE Reginald (ENG)\nMITCHELL George (SCO)\nDA COSTA VIEIRA Jose (POR)\n209.0\n1186.0\nBRA\nESP\n\n\n77\n1954\n16 Jun 1954 - 18:00\nGroup 1\nCharmilles\nGeneva\nBrazil\n5.0\n0.0\nMexico\n\n13470.0\n4.0\n0.0\nWYSSLING Paul (SUI)\nSCHONHOLZER Ernest (SUI)\nDA COSTA VIEIRA Jose (POR)\n211.0\n1249.0\nBRA\nMEX\n\n\n133\n1958\n24 Jun 1958 - 19:00\nSemi-finals\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nFrance\n\n27100.0\n2.0\n1.0\nGRIFFITHS Benjamin (WAL)\nWYSSLING Paul (SUI)\nLEAFE Reginald (ENG)\n488.0\n1340.0\nBRA\nFRA\n\n\n135\n1958\n29 Jun 1958 - 15:00\nFinal\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nSweden\n\n49737.0\n2.0\n1.0\nGUIGUE Maurice (FRA)\nDUSCH Albert (GER)\nGARDEAZABAL Juan (ESP)\n3482.0\n1343.0\nBRA\nSWE\n\n\n\n\n\n\n\nIt is very similar to what we would using loc, and they are essencially the same thing:\n\ndf.loc[(df.home_team_initials == \"BRA\") & (df.home_team_goals &gt;= 5)]\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\nattendance\nhalf-time_home_goals\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\n\n\n\n\n40\n1938\n05 Jun 1938 - 17:30\nFirst round\nStade de la Meinau\nStrasbourg\nBrazil\n6.0\n5.0\nPoland\nBrazil win after extra time\n13452.0\n0.0\n0.0\nEKLIND Ivan (SWE)\nPOISSANT Louis (FRA)\nKISSENBERGER Ernest (FRA)\n206.0\n1150.0\nBRA\nPOL\n\n\n70\n1950\n09 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n7.0\n1.0\nSweden\n\n138886.0\n3.0\n0.0\nELLIS Arthur (ENG)\nGARCIA Prudencio (USA)\nDE LA SALLE Charles (FRA)\n209.0\n1189.0\nBRA\nSWE\n\n\n71\n1950\n13 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n6.0\n1.0\nSpain\n\n152772.0\n3.0\n0.0\nLEAFE Reginald (ENG)\nMITCHELL George (SCO)\nDA COSTA VIEIRA Jose (POR)\n209.0\n1186.0\nBRA\nESP\n\n\n77\n1954\n16 Jun 1954 - 18:00\nGroup 1\nCharmilles\nGeneva\nBrazil\n5.0\n0.0\nMexico\n\n13470.0\n4.0\n0.0\nWYSSLING Paul (SUI)\nSCHONHOLZER Ernest (SUI)\nDA COSTA VIEIRA Jose (POR)\n211.0\n1249.0\nBRA\nMEX\n\n\n133\n1958\n24 Jun 1958 - 19:00\nSemi-finals\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nFrance\n\n27100.0\n2.0\n1.0\nGRIFFITHS Benjamin (WAL)\nWYSSLING Paul (SUI)\nLEAFE Reginald (ENG)\n488.0\n1340.0\nBRA\nFRA\n\n\n135\n1958\n29 Jun 1958 - 15:00\nFinal\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nSweden\n\n49737.0\n2.0\n1.0\nGUIGUE Maurice (FRA)\nDUSCH Albert (GER)\nGARDEAZABAL Juan (ESP)\n3482.0\n1343.0\nBRA\nSWE\n\n\n\n\n\n\n\nI feel that mostly the advantages of using query are related to readability and general code organization. I just think is a nicer, and often, a faster way to write your code.\nIf we want to reference variables in the environment, we can achieve that with query using @ before it, such as in:\n\nmin_goals = 5\ndf.query('home_team_initials == \"BRA\" & home_team_goals &gt;= @min_goals')\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\nattendance\nhalf-time_home_goals\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\n\n\n\n\n40\n1938\n05 Jun 1938 - 17:30\nFirst round\nStade de la Meinau\nStrasbourg\nBrazil\n6.0\n5.0\nPoland\nBrazil win after extra time\n13452.0\n0.0\n0.0\nEKLIND Ivan (SWE)\nPOISSANT Louis (FRA)\nKISSENBERGER Ernest (FRA)\n206.0\n1150.0\nBRA\nPOL\n\n\n70\n1950\n09 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n7.0\n1.0\nSweden\n\n138886.0\n3.0\n0.0\nELLIS Arthur (ENG)\nGARCIA Prudencio (USA)\nDE LA SALLE Charles (FRA)\n209.0\n1189.0\nBRA\nSWE\n\n\n71\n1950\n13 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n6.0\n1.0\nSpain\n\n152772.0\n3.0\n0.0\nLEAFE Reginald (ENG)\nMITCHELL George (SCO)\nDA COSTA VIEIRA Jose (POR)\n209.0\n1186.0\nBRA\nESP\n\n\n77\n1954\n16 Jun 1954 - 18:00\nGroup 1\nCharmilles\nGeneva\nBrazil\n5.0\n0.0\nMexico\n\n13470.0\n4.0\n0.0\nWYSSLING Paul (SUI)\nSCHONHOLZER Ernest (SUI)\nDA COSTA VIEIRA Jose (POR)\n211.0\n1249.0\nBRA\nMEX\n\n\n133\n1958\n24 Jun 1958 - 19:00\nSemi-finals\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nFrance\n\n27100.0\n2.0\n1.0\nGRIFFITHS Benjamin (WAL)\nWYSSLING Paul (SUI)\nLEAFE Reginald (ENG)\n488.0\n1340.0\nBRA\nFRA\n\n\n135\n1958\n29 Jun 1958 - 15:00\nFinal\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nSweden\n\n49737.0\n2.0\n1.0\nGUIGUE Maurice (FRA)\nDUSCH Albert (GER)\nGARDEAZABAL Juan (ESP)\n3482.0\n1343.0\nBRA\nSWE\n\n\n\n\n\n\n\nWith query, we can also use the in operator to select only the values within a list:\n\nteams = ['BRA', 'FRA']\ndf.query('home_team_initials in @teams & home_team_goals &gt;= @min_goals')\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\nattendance\nhalf-time_home_goals\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\n\n\n\n\n40\n1938\n05 Jun 1938 - 17:30\nFirst round\nStade de la Meinau\nStrasbourg\nBrazil\n6.0\n5.0\nPoland\nBrazil win after extra time\n13452.0\n0.0\n0.0\nEKLIND Ivan (SWE)\nPOISSANT Louis (FRA)\nKISSENBERGER Ernest (FRA)\n206.0\n1150.0\nBRA\nPOL\n\n\n70\n1950\n09 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n7.0\n1.0\nSweden\n\n138886.0\n3.0\n0.0\nELLIS Arthur (ENG)\nGARCIA Prudencio (USA)\nDE LA SALLE Charles (FRA)\n209.0\n1189.0\nBRA\nSWE\n\n\n71\n1950\n13 Jul 1950 - 15:00\nGroup 6\nMaracan� - Est�dio Jornalista M�rio Filho\nRio De Janeiro\nBrazil\n6.0\n1.0\nSpain\n\n152772.0\n3.0\n0.0\nLEAFE Reginald (ENG)\nMITCHELL George (SCO)\nDA COSTA VIEIRA Jose (POR)\n209.0\n1186.0\nBRA\nESP\n\n\n77\n1954\n16 Jun 1954 - 18:00\nGroup 1\nCharmilles\nGeneva\nBrazil\n5.0\n0.0\nMexico\n\n13470.0\n4.0\n0.0\nWYSSLING Paul (SUI)\nSCHONHOLZER Ernest (SUI)\nDA COSTA VIEIRA Jose (POR)\n211.0\n1249.0\nBRA\nMEX\n\n\n105\n1958\n08 Jun 1958 - 19:00\nGroup 2\nIdrottsparken\nNorrk�Ping\nFrance\n7.0\n3.0\nParaguay\n\n16518.0\n2.0\n2.0\nGARDEAZABAL Juan (ESP)\nGRIFFITHS Benjamin (WAL)\nBROZZI Juan (ARG)\n220.0\n1386.0\nFRA\nPAR\n\n\n133\n1958\n24 Jun 1958 - 19:00\nSemi-finals\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nFrance\n\n27100.0\n2.0\n1.0\nGRIFFITHS Benjamin (WAL)\nWYSSLING Paul (SUI)\nLEAFE Reginald (ENG)\n488.0\n1340.0\nBRA\nFRA\n\n\n134\n1958\n28 Jun 1958 - 17:00\nMatch for third place\nNya Ullevi\nGothenburg\nFrance\n6.0\n3.0\nGermany FR\n\n32483.0\n3.0\n1.0\nBROZZI Juan (ARG)\nELLIS Arthur (ENG)\nLUNDELL Bengt (SWE)\n3483.0\n1382.0\nFRA\nFRG\n\n\n135\n1958\n29 Jun 1958 - 15:00\nFinal\nRasunda Stadium\nSolna\nBrazil\n5.0\n2.0\nSweden\n\n49737.0\n2.0\n1.0\nGUIGUE Maurice (FRA)\nDUSCH Albert (GER)\nGARDEAZABAL Juan (ESP)\n3482.0\n1343.0\nBRA\nSWE"
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#assign",
    "href": "posts/everyday-pandas/everyday_pandas.html#assign",
    "title": "Everyday pandas",
    "section": "assign",
    "text": "assign\nHere is what we can find about the assign function on the pandas documentation:\nReturns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten.\nSo assign is literally just a different way to assign new columns to our dataframe, but really the great thing is that we can create multiple columns at once, in which a new column can also depend on one that was just created.\nLet’s use as an example a case where we want to query the matches called a “big goal match”. This type of match represents the games in which the total number of goals were bigger than 6. One way to go about that is to create a new column total_goals that sums the goals scored by the home team and the away team, and create a new column big_goals_match that can be either 0 or 1 to flag if the game was considered a “big goal match”.\nHere is how we can perform this with assign:\n\ngoals_matches = df.assign(\n                    total_goals     = lambda d: d['home_team_goals'] + d['away_team_goals'], \n                    big_goals_match = lambda d: (d['total_goals'] &gt; 5).astype('int')\n                )[['home_team_initials', 'away_team_initials', 'home_team_goals', 'away_team_goals', 'total_goals', 'big_goals_match']]\n\ngoals_matches.query('big_goals_match == 1').head()\n\n\n\n\n\n\n\n\nhome_team_initials\naway_team_initials\nhome_team_goals\naway_team_goals\ntotal_goals\nbig_goals_match\n\n\n\n\n10\nARG\nMEX\n6.0\n3.0\n9.0\n1\n\n\n15\nARG\nUSA\n6.0\n1.0\n7.0\n1\n\n\n16\nURU\nYUG\n6.0\n1.0\n7.0\n1\n\n\n17\nURU\nARG\n4.0\n2.0\n6.0\n1\n\n\n19\nHUN\nEGY\n4.0\n2.0\n6.0\n1\n\n\n\n\n\n\n\n\nWhat if we wanted to get the top 10 matches that are a “big goal match” with the smallest difference between the home and away team?\nBefore going straight to the code, one thing I learned in pandas was to organize my code within the parenthesis so that we can escape lines when adding new methods to the existing dataframe. In a code like this, you know very easily what is being computed at each step. It’s easier to read, and in case you want to test something new, you can simply comment or uncomment an existing line:\n\n(df.assign( \n    # Add new columns to existing dataframe \n        total_goals =     lambda d: d['home_team_goals'] + d['away_team_goals'], \n        big_goals_match = lambda d: (d['total_goals'] &gt; 5).astype('int'), \n        goal_diff =       lambda d: abs(d['home_team_goals'] - d['away_team_goals'])\n    )\n\n    # Select only the big goals match \n    .query('big_goals_match == 1')\n\n    # sort values by goal_diff\n    .sort_values('goal_diff', ascending = True)\n\n    # select the first 10 lines\n    .head(10)\n\n    # Reset index so it numbers the data from 0 to 9\n    .reset_index(drop = True)\n\n    # Pick important columns for display\n    [['home_team_initials', 'away_team_initials', 'home_team_goals', 'away_team_goals', 'total_goals', 'big_goals_match', 'goal_diff']]\n)\n\n\n\n\n\n\n\n\nhome_team_initials\naway_team_initials\nhome_team_goals\naway_team_goals\ntotal_goals\nbig_goals_match\ngoal_diff\n\n\n\n\n0\nURS\nCOL\n4.0\n4.0\n8.0\n1\n0.0\n\n\n1\nENG\nBEL\n4.0\n4.0\n8.0\n1\n0.0\n\n\n2\nFRG\nFRA\n3.0\n3.0\n6.0\n1\n0.0\n\n\n3\nPAR\nYUG\n3.0\n3.0\n6.0\n1\n0.0\n\n\n4\nSEN\nURU\n3.0\n3.0\n6.0\n1\n0.0\n\n\n5\nCUB\nROU\n3.0\n3.0\n6.0\n1\n0.0\n\n\n6\nBRA\nPOL\n6.0\n5.0\n11.0\n1\n1.0\n\n\n7\nURS\nBEL\n3.0\n4.0\n7.0\n1\n1.0\n\n\n8\nITA\nFRG\n4.0\n3.0\n7.0\n1\n1.0\n\n\n9\nURU\nENG\n4.0\n2.0\n6.0\n1\n2.0"
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#cut-and-qcut",
    "href": "posts/everyday-pandas/everyday_pandas.html#cut-and-qcut",
    "title": "Everyday pandas",
    "section": "cut and qcut",
    "text": "cut and qcut\nVery often when we run some type of analysis, it’s interesting to see how the data is distributed acording to a certain variable. A nice way to do this is to simply add bins to the data, and pandas makes it very easy to do that with cut and qcut. The difference between them is that cut let’s you specify the exact bin edges that you want to cut the data, and qcut uses the quantiles to do that. I personally use qcut way more often, since it already splits the data in equal size bins.\nLet’s try to answer the following question: “Are the matches in the top 10% attendance of all time made only by the final stages in the World Cup?”\nTo answer this, let’s first create the bins with qcut. In order to understand what is the top 10% in attendence, we need to split the data in 10 equally sized parts, here is how we can do that:\n\nlabels_bins = [f'p{x}' for x in range(0, 100, 10)]\n\ndf = (df.assign(\n        bins_attendance = lambda d: pd.qcut(d['attendance'], q = 10, labels = labels_bins)\n    )\n)\n\n# Check the distribution for each bin\ndf.bins_attendance.value_counts(normalize = True)\n\np60    0.101796\np0     0.100599\np20    0.100599\np40    0.100599\np90    0.100599\np10    0.099401\np30    0.099401\np50    0.099401\np80    0.099401\np70    0.098204\nName: bins_attendance, dtype: float64\n\n\nAs expected, each bin has around 10% of the data\nNow, answering the question:\n\nhighest_attendence = (df.groupby(['stage'])\n                        ['bins_attendance']\n                        .value_counts()\n                        .reset_index(name = 'counts')\n                        .query('bins_attendance == \"p90\" & counts &gt; 0')\n                        .sort_values('counts', ascending = False)\n                        .reset_index(drop = True)\n                    )\n\nhighest_attendence\n\n\n\n\n\n\n\n\nstage\nbins_attendance\ncounts\n\n\n\n\n0\nGroup 1\np90\n12\n\n\n1\nGroup B\np90\n11\n\n\n2\nFinal\np90\n9\n\n\n3\nSemi-finals\np90\n8\n\n\n4\nGroup A\np90\n7\n\n\n5\nRound of 16\np90\n7\n\n\n6\nQuarter-finals\np90\n6\n\n\n7\nGroup 2\np90\n4\n\n\n8\nGroup E\np90\n4\n\n\n9\nMatch for third place\np90\n4\n\n\n10\nGroup 6\np90\n3\n\n\n11\nGroup F\np90\n2\n\n\n12\nGroup G\np90\n2\n\n\n13\nGroup D\np90\n2\n\n\n14\nGroup 3\np90\n1\n\n\n15\nGroup C\np90\n1\n\n\n16\nGroup H\np90\n1\n\n\n\n\n\n\n\nAs we can see, the top 10% in attendence is made by lots of different World Cup stages, including the first ones."
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#pivot-table",
    "href": "posts/everyday-pandas/everyday_pandas.html#pivot-table",
    "title": "Everyday pandas",
    "section": "Pivot table",
    "text": "Pivot table\nPivot table is a very common transformation. If you know excel, you’re probably already familiar with it and use it a lot as well. This method is very useful when we want to check how two variables (or more) affect another one with a custom statistic. Sounds a bit confusing, so let’s illustrate that with this question:\n\nHow many goals each stage had on average in the last 3 world cups?\n\nSo what we want would be a dataframe that had columns as the years (indicading the last 3 world cups), the index as the stages, and the values as the mean goals scored in each stage/year. There are different ways to construct this, but pivot_table really makes it easy, here’s how to do that:\n\n# Add column with the total number of goals scored \ndf = df.assign(\n    total_goals = lambda d: d['home_team_goals'] + d['away_team_goals']\n)\n\n# Build pivot table\ndf_avg_goals = (pd.pivot_table(\n                        df.query('year &gt;= 2006'), \n                        columns = 'year', \n                        index   = 'stage', \n                        values  = 'total_goals', \n                        aggfunc = 'mean'\n                    ).round(2)\n                )\n\ndf_avg_goals\n\n\n\n\n\n\n\nyear\n2006\n2010\n2014\n\n\nstage\n\n\n\n\n\n\n\nFinal\n2.00\n1.00\n1.00\n\n\nGroup A\n3.00\n1.83\n3.00\n\n\nGroup B\n1.67\n2.83\n3.67\n\n\nGroup C\n3.00\n1.50\n2.83\n\n\nGroup D\n2.00\n2.00\n2.00\n\n\nGroup E\n2.33\n2.33\n3.17\n\n\nGroup F\n2.67\n2.17\n2.33\n\n\nGroup G\n1.83\n2.83\n3.17\n\n\nGroup H\n3.00\n1.33\n2.50\n\n\nMatch for third place\nNaN\n5.00\nNaN\n\n\nPlay-off for third place\nNaN\nNaN\n3.00\n\n\nQuarter-finals\n1.50\n2.50\n1.25\n\n\nRound of 16\n1.88\n2.75\n2.14\n\n\nSemi-finals\n1.50\n3.00\n4.00\n\n\nThird place\n4.00\nNaN\nNaN\n\n\n\n\n\n\n\n\nstyle - never run out of it!\nIt’s hard to collect insights looking at the last table quickly. With style we can set different ways to color our dataframe. To simply create a color scale, we can use the method background_gradient:\n\n(df_avg_goals\n    .style\n    .format(na_rep='', precision=2)\n    .background_gradient()\n)\n\n\n\n\n\n\nyear\n2006\n2010\n2014\n\n\nstage\n \n \n \n\n\n\n\nFinal\n2.00\n1.00\n1.00\n\n\nGroup A\n3.00\n1.83\n3.00\n\n\nGroup B\n1.67\n2.83\n3.67\n\n\nGroup C\n3.00\n1.50\n2.83\n\n\nGroup D\n2.00\n2.00\n2.00\n\n\nGroup E\n2.33\n2.33\n3.17\n\n\nGroup F\n2.67\n2.17\n2.33\n\n\nGroup G\n1.83\n2.83\n3.17\n\n\nGroup H\n3.00\n1.33\n2.50\n\n\nMatch for third place\n\n5.00\n\n\n\nPlay-off for third place\n\n\n3.00\n\n\nQuarter-finals\n1.50\n2.50\n1.25\n\n\nRound of 16\n1.88\n2.75\n2.14\n\n\nSemi-finals\n1.50\n3.00\n4.00\n\n\nThird place\n4.00\n\n\n\n\n\n\n\nMuch better!\nThe Style object from pandas has a lot of different ways to customize a dataframe. The background_gradient method already satisfies most of my needs for styling, but if you want to check more options, the documentation has a lot of great examples."
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#pipe---putting-it-all-together",
    "href": "posts/everyday-pandas/everyday_pandas.html#pipe---putting-it-all-together",
    "title": "Everyday pandas",
    "section": "pipe - putting it all together!",
    "text": "pipe - putting it all together!\nI first learned about this method with calmcode.io. There is a series of very nice videos that I definitely recommend checking out!\nWith pipe, we can create several of different functions to process our dataset. The idea is that each function should be responsible for one step of the dataset processing. For example, it’s very common to fix the data types, then add some new variables to the data, perform some type of scaling, etc. So, instead of doing something like this:\ndf1 = df.astype(type_schema)\n\ndf2 = df1.copy()\ndf2['total_goals'] = df2['home_team_goals'] + df2['away_team_goals']\n\ndf3 = df2.copy()\ndata_scaled = scaler.fit_transform(df3[feature_list])\nWe can do something like this:\ndef fix_dtypes(dataf, type_schema): \n    return dataf.astype(type_schema)\n\ndef add_total_goals(dataf): \n    return dataf.assign(\n        lambda d: d['home_team_goals'] + d['away_team_goals']\n    )\n\ndef scale_dataset(dataf, features): \n    X = scaler.fit_transform(dataf[features])\n    return pd.DataFrame(X, index = dataf.index, columns = features)\nAnd then simply call pipe to chain the functions:\ndata_scaled = (df.pipe(fix_dtypes, type_schema = type_schema)\n                 .pipe(add_total_goals)\n                 .pipe(scale_dataset, features = feature_list)\n)\nThis can bring a very nice touch to our code for a few reasons:\n\nReadability: It’s a lot easier to understand and also organize the code. By creating functions for each step, you will probabily spend a bit more time trying to make it nicer and reproducible, rather than just creating a lot of different dataframes for every tiny change you want to make.\nPractical: If you’re running different tests, you can simply comment/uncomment a line and all the other transformations that you need will still be there.\nClean code: If you have common steps for several notebooks, you can add the functions to a python file and then import them to the notebook. The code will be a lot cleaner and the notebook will have only the analysis/plots/modeling/etc. In my experience, pipe is specially useful when building the preprocessing steps for a Machine Learning model.\n\nNow that we have all that in mind, let’s try to answer this question to put everything we learned together:\n\nOn average, a team that won the World Cup scores how many more goals than other teams?\n\nBefore anything, let’s take a look at our data again:\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\ndatetime\nstage\nstadium\ncity\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\n...\nhalf-time_away_goals\nreferee\nassistant_1\nassistant_2\nroundid\nmatchid\nhome_team_initials\naway_team_initials\nbins_attendance\ntotal_goals\n\n\n\n\n0\n1930\n13 Jul 1930 - 15:00\nGroup 1\nPocitos\nMontevideo\nFrance\n4.0\n1.0\nMexico\n\n...\n0.0\nLOMBARDI Domingo (URU)\nCRISTOPHE Henry (BEL)\nREGO Gilberto (BRA)\n201.0\n1096.0\nFRA\nMEX\np0\n5.0\n\n\n1\n1930\n13 Jul 1930 - 15:00\nGroup 4\nParque Central\nMontevideo\nUSA\n3.0\n0.0\nBelgium\n\n...\n0.0\nMACIAS Jose (ARG)\nMATEUCCI Francisco (URU)\nWARNKEN Alberto (CHI)\n201.0\n1090.0\nUSA\nBEL\np10\n3.0\n\n\n2\n1930\n14 Jul 1930 - 12:45\nGroup 2\nParque Central\nMontevideo\nYugoslavia\n2.0\n1.0\nBrazil\n\n...\n0.0\nTEJADA Anibal (URU)\nVALLARINO Ricardo (URU)\nBALWAY Thomas (FRA)\n201.0\n1093.0\nYUG\nBRA\np10\n3.0\n\n\n3\n1930\n14 Jul 1930 - 14:50\nGroup 3\nPocitos\nMontevideo\nRomania\n3.0\n1.0\nPeru\n\n...\n0.0\nWARNKEN Alberto (CHI)\nLANGENUS Jean (BEL)\nMATEUCCI Francisco (URU)\n201.0\n1098.0\nROU\nPER\np0\n4.0\n\n\n4\n1930\n15 Jul 1930 - 16:00\nGroup 1\nParque Central\nMontevideo\nArgentina\n1.0\n0.0\nFrance\n\n...\n0.0\nREGO Gilberto (BRA)\nSAUCEDO Ulises (BOL)\nRADULESCU Constantin (ROU)\n201.0\n1085.0\nARG\nFRA\np10\n1.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nOne way to go about this, is to create a dataframe that contains how many goals each team scored in the world cup, add a column is_world_cup_winner and then compare the winners with the others.\nTo identify the winners, we can check who won the final match by comparing the number of goals, but the problem with the dataset is that when the final was decided on the penalties, the winner will be on the description win_conditions:\n\ndf.query('year == 2006 & stage == \"Final\"')[['home_team_name',\n                                             'home_team_goals',\n                                             'away_team_goals', \n                                             'away_team_name', \n                                             'win_conditions'\n                                             ]]\n\n\n\n\n\n\n\n\nhome_team_name\nhome_team_goals\naway_team_goals\naway_team_name\nwin_conditions\n\n\n\n\n707\nItaly\n1.0\n1.0\nFrance\nItaly win on penalties (5 - 3)\n\n\n\n\n\n\n\nSo, we will also have to correct for that.\nHere is the pipeline we will build: - start_pipe: just start by copying the dataframe since the pipe method modifies the original df - total_goals_by_year: create a df with how many goals each country scored each year - world_cup_winners: query the countries that were in the “Final” stage in each year and select the country with more goals. Then, apply correct_winners to correct the winner country when the World Cup was decided on the penalties:\n\ndef start_pipe(dataf): \n    return dataf.copy()\n    \ndef total_goals_by_year(dataf):\n    away_team_goals = (dataf.groupby(['year', 'away_team_name'])\n                        ['away_team_goals']\n                        .sum()\n                        .to_frame()\n                    )\n    home_team_goals = (dataf.groupby(['year', 'home_team_name'])\n                        ['home_team_goals']\n                        .sum()\n                        .to_frame()\n                    )\n    \n    return (pd.concat([away_team_goals,\n                       home_team_goals], axis = 1\n                ).fillna(0)\n                .assign(total_goals = lambda d: d['away_team_goals'] + d['home_team_goals'])\n            )[['total_goals']]\n\ndef world_cup_winners(dataf): \n   return (dataf.query('stage == \"Final\"')\n                .assign(winner_team = lambda d: d.apply(\n                                    lambda x: x['home_team_name'] if x['home_team_goals'] &gt; x['away_team_goals'] else x['away_team_name'], axis = 1\n                                )\n                        )\n        )\n\ndef correct_winners(description): \n    match = re.search(r'\\b(\\w+)\\s+win\\b', description)\n    if match != None: \n        return match.group(1)\n    else: \n        return ''\n    \ndef wc_winners_corrected(dataf): \n    return (dataf.assign(teams_corrected = lambda d: d.win_conditions.apply(correct_winners), \n                         winner_team_corrected = lambda d: d.apply(\n                            lambda x: x['winner_team'] if x['teams_corrected'] == '' else x['teams_corrected'], axis = 1\n                        )\n\n                    ).reset_index(drop = True)\n    )[['year', 'winner_team', 'teams_corrected', 'winner_team_corrected']]\n\n\nwinners = (df.pipe(start_pipe)\n            .pipe(world_cup_winners)\n            .pipe(wc_winners_corrected)\n         )\n\nwinners.tail()\n\n\n\n\n\n\n\n\nyear\nwinner_team\nteams_corrected\nwinner_team_corrected\n\n\n\n\n14\n1998\nFrance\n\nFrance\n\n\n15\n2002\nBrazil\n\nBrazil\n\n\n16\n2006\nFrance\nItaly\nItaly\n\n\n17\n2010\nSpain\nSpain\nSpain\n\n\n18\n2014\nGermany\nGermany\nGermany\n\n\n\n\n\n\n\n\ncountry_goals_by_year = (df.pipe(start_pipe)\n                           .pipe(total_goals_by_year)).sort_index()\n\ncountry_goals_by_year.tail()\n\n\n\n\n\n\n\n\n\ntotal_goals\n\n\nyear\n\n\n\n\n\n\n2014\nSpain\n4.0\n\n\nSwitzerland\n7.0\n\n\nUSA\n5.0\n\n\nUruguay\n4.0\n\n\nrn\"&gt;Bosnia and Herzegovina\n4.0\n\n\n\n\n\n\n\nFinally, we can merge the datasets so we have how many goals were scored by each winner team:\n\nwinners_goals = (   # merge winner's information with total goals by each country on that year\n                    pd.merge(\n                        winners[['year', 'winner_team_corrected']],\n                        (country_goals_by_year.reset_index()\n                            .rename(columns = {'level_1':'team_name'})\n                        ),                     \n                        on = 'year'\n                    )\n                    # assign a flag to world cup winners based on winners df \n                    .assign(is_world_cup_winner = lambda d: (d['team_name'] == d['winner_team_corrected']).astype('int'))\n\n                    # keep relevant columns\n                    [['year', 'team_name', 'total_goals', 'is_world_cup_winner']]\n            )\nwinners_goals.head()\n\n\n\n\n\n\n\n\nyear\nteam_name\ntotal_goals\nis_world_cup_winner\n\n\n\n\n0\n1930\nArgentina\n18.0\n0\n\n\n1\n1930\nBelgium\n0.0\n0\n\n\n2\n1930\nBolivia\n0.0\n0\n\n\n3\n1930\nBrazil\n5.0\n0\n\n\n4\n1930\nChile\n5.0\n0\n\n\n\n\n\n\n\nNow, we can simply calculate the mean total goals by a team when they have won the World Cup:\n\navg_goals = (winners_goals\n                .groupby('is_world_cup_winner')\n                [['total_goals']]\n                .mean().T\n            )\n\navg_goals\n\n\n\n\n\n\n\nis_world_cup_winner\n0\n1\n\n\n\n\ntotal_goals\n5.103797\n14.315789\n\n\n\n\n\n\n\n\navg_diff_goals = (avg_goals[1] - avg_goals[0]).iloc[0].round(2)\n\nprint(f'On average, a team that won the World Cup scores {avg_diff_goals} more goals than other teams')\n\nOn average, a team that won the World Cup scores 9.21 more goals than other teams"
  },
  {
    "objectID": "posts/everyday-pandas/everyday_pandas.html#final-thoughts",
    "href": "posts/everyday-pandas/everyday_pandas.html#final-thoughts",
    "title": "Everyday pandas",
    "section": "Final thoughts",
    "text": "Final thoughts\nPandas is for sure one of the most important libraries I work with everyday, and still I keep finding new ways to use it. There are a dozen other topics we didn’t cover here, but I just wanted to do a general overview on some of the very important things I learned with the library and that really help me on a daily basis.\nIf you are looking for more resources, a lot of the ideas here come from the very good Modern Pandas writing series by Tom Augspurger. For portuguese speakers, I also have a post on my Medium page about pandas for Time Series analysis."
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html",
    "href": "posts/hillstrom-challenge/meta_models.html",
    "title": "About the data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom fklearn.causal.validation.curves import relative_cumulative_gain_curve\nfrom fklearn.causal.validation.auc import area_under_the_relative_cumulative_gain_curve\nThis dataset contains 64,000 customers who last purchased within twelve months. The customers were involved in an e-mail test. - 1/3 were randomly chosen to receive an e-mail campaign featuring Mens merchandise. - 1/3 were randomly chosen to receive an e-mail campaign featuring Womens merchandise. - 1/3 were randomly chosen to not receive an e-mail campaign."
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#the-challenge",
    "href": "posts/hillstrom-challenge/meta_models.html#the-challenge",
    "title": "About the data",
    "section": "The challenge",
    "text": "The challenge\n\nWhich e-mail campaign performed the best, the Mens version, or the Womens version?\nHow much incremental sales per customer did the Mens version of the e-mail campaign drive? How much incremental sales per customer did the Womens version of the e-mail campaign drive?\nIf you could only send an e-mail campaign to the best 10,000 customers, which customers would receive the e-mail campaign? Why?\nIf you had to eliminate 10,000 customers from receiving an e-mail campaign, which customers would you suppress from the campaign? Why?\nDid the Mens version of the e-mail campaign perform different than the Womens version of the e-mail campaign, across various customer segments?\nDid the campaigns perform different when measured across different metrics, like Visitors, Conversion, and Total Spend?\nDid you observe any anomalies, or odd findings?\nWhich audience would you target the Mens version to, and the Womens version to, given the results of the test? What data do you have to support your recommendation?"
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#explore-data",
    "href": "posts/hillstrom-challenge/meta_models.html#explore-data",
    "title": "About the data",
    "section": "Explore data",
    "text": "Explore data\n\ndf = pd.read_csv('hillstrom.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nrecency\nhistory_segment\nhistory\nmens\nwomens\nzip_code\nnewbie\nchannel\nsegment\nvisit\nconversion\nspend\n\n\n\n\n0\n10\n2) $100 - $200\n142.44\n1\n0\nSurburban\n0\nPhone\nWomens E-Mail\n0\n0\n0.0\n\n\n1\n6\n3) $200 - $350\n329.08\n1\n1\nRural\n1\nWeb\nNo E-Mail\n0\n0\n0.0\n\n\n2\n7\n2) $100 - $200\n180.65\n0\n1\nSurburban\n1\nWeb\nWomens E-Mail\n0\n0\n0.0\n\n\n3\n9\n5) $500 - $750\n675.83\n1\n0\nRural\n1\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n4\n2\n1) $0 - $100\n45.34\n1\n0\nUrban\n0\nWeb\nWomens E-Mail\n0\n0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63995\n10\n2) $100 - $200\n105.54\n1\n0\nUrban\n0\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n63996\n5\n1) $0 - $100\n38.91\n0\n1\nUrban\n1\nPhone\nMens E-Mail\n0\n0\n0.0\n\n\n63997\n6\n1) $0 - $100\n29.99\n1\n0\nUrban\n1\nPhone\nMens E-Mail\n0\n0\n0.0\n\n\n63998\n1\n5) $500 - $750\n552.94\n1\n0\nSurburban\n1\nMultichannel\nWomens E-Mail\n0\n0\n0.0\n\n\n63999\n1\n4) $350 - $500\n472.82\n0\n1\nSurburban\n0\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n\n\n64000 rows × 12 columns\n\n\n\n\ndf.columns\n\nIndex(['recency', 'history_segment', 'history', 'mens', 'womens', 'zip_code',\n       'newbie', 'channel', 'segment', 'visit', 'conversion', 'spend'],\n      dtype='object')\n\n\n\nfor v in ['recency', 'history_segment', 'mens', 'womens', 'zip_code','newbie', 'channel', 'segment', 'visit', 'conversion']: \n    display(df.groupby(v)\n        .history.describe(percentiles = [.1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99])\n        .style.background_gradient()\n        )\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nrecency\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n1\n8952.000000\n359.844955\n350.504790\n29.990000\n38.691000\n82.266000\n131.134000\n186.746000\n252.960000\n331.162000\n439.077000\n581.190000\n812.696000\n1058.663000\n1623.616300\n3215.970000\n\n\n2\n7537.000000\n310.990777\n314.524776\n29.990000\n31.506000\n68.440000\n107.988000\n155.416000\n211.390000\n276.220000\n371.124000\n496.104000\n719.794000\n953.230000\n1451.976400\n3345.930000\n\n\n3\n5904.000000\n274.462007\n273.244529\n29.990000\n29.990000\n61.994000\n99.318000\n139.592000\n187.485000\n246.636000\n325.840000\n435.848000\n628.574000\n820.283000\n1273.357500\n2809.790000\n\n\n4\n5077.000000\n251.310701\n247.867615\n29.990000\n29.990000\n55.158000\n85.626000\n123.730000\n172.220000\n228.616000\n304.414000\n407.104000\n584.342000\n745.564000\n1171.386800\n2047.640000\n\n\n5\n4510.000000\n233.153233\n230.218266\n29.990000\n29.990000\n49.780000\n79.436000\n116.370000\n155.865000\n206.416000\n282.957000\n378.700000\n538.486000\n694.244500\n1037.106400\n1802.300000\n\n\n6\n4605.000000\n218.387238\n219.884675\n29.990000\n29.990000\n46.758000\n74.570000\n106.740000\n145.720000\n196.998000\n258.568000\n350.158000\n497.640000\n654.874000\n1000.958800\n2583.840000\n\n\n7\n4078.000000\n207.557580\n203.493936\n29.990000\n29.990000\n46.202000\n73.432000\n105.840000\n143.435000\n187.380000\n251.230000\n332.754000\n468.712000\n607.408000\n937.157000\n2816.010000\n\n\n8\n3495.000000\n194.924066\n191.524765\n29.990000\n29.990000\n41.658000\n66.244000\n95.560000\n132.920000\n175.192000\n230.624000\n307.640000\n460.874000\n587.184000\n885.347800\n1422.370000\n\n\n9\n6441.000000\n185.874574\n179.512854\n29.990000\n29.990000\n40.520000\n66.270000\n93.560000\n128.140000\n169.060000\n219.630000\n296.230000\n420.570000\n552.710000\n837.346000\n1507.750000\n\n\n10\n7565.000000\n172.104997\n167.600012\n29.990000\n29.990000\n38.434000\n59.822000\n86.752000\n116.910000\n154.822000\n206.004000\n270.426000\n388.262000\n502.456000\n790.850000\n1730.730000\n\n\n11\n3504.000000\n168.015759\n160.202071\n29.990000\n29.990000\n37.842000\n59.082000\n86.678000\n118.470000\n155.670000\n200.897000\n264.582000\n375.704000\n481.520000\n749.627400\n1428.700000\n\n\n12\n2332.000000\n153.984108\n155.478535\n29.990000\n29.990000\n35.208000\n54.960000\n75.218000\n101.855000\n134.486000\n175.841000\n237.778000\n345.743000\n473.900000\n755.038900\n1442.600000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nhistory_segment\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n1) $0 - $100\n22970.000000\n51.650579\n22.663717\n29.990000\n29.990000\n29.990000\n29.990000\n35.036000\n44.545000\n54.470000\n64.990000\n76.060000\n87.620000\n93.650000\n98.803100\n99.990000\n\n\n2) $100 - $200\n14254.000000\n146.452272\n28.761238\n100.000000\n108.063000\n116.716000\n125.990000\n135.126000\n144.765000\n154.658000\n165.450000\n176.420000\n187.600000\n193.813500\n198.780000\n199.980000\n\n\n3) $200 - $350\n12289.000000\n266.900125\n43.208864\n200.000000\n211.358000\n222.596000\n234.740000\n248.482000\n262.490000\n278.198000\n294.590000\n311.874000\n330.210000\n339.990000\n347.960000\n349.960000\n\n\n4) $350 - $500\n6409.000000\n417.787166\n43.201604\n350.010000\n360.990000\n373.772000\n386.714000\n399.680000\n414.560000\n428.748000\n445.524000\n462.996000\n481.050000\n490.000000\n498.060000\n499.950000\n\n\n5) $500 - $750\n4911.000000\n604.636854\n70.567936\n500.000000\n516.410000\n533.720000\n552.000000\n573.140000\n595.380000\n618.660000\n646.280000\n674.860000\n711.820000\n731.080000\n745.728000\n749.790000\n\n\n6) $750 - $1,000\n1859.000000\n853.769042\n70.017433\n750.010000\n768.048000\n784.814000\n802.734000\n821.860000\n841.760000\n866.988000\n892.824000\n927.322000\n960.012000\n977.959000\n995.329600\n999.750000\n\n\n7) $1,000 +\n1308.000000\n1303.880482\n306.597516\n1000.150000\n1033.470000\n1072.320000\n1113.115000\n1161.172000\n1214.500000\n1277.020000\n1362.426000\n1481.148000\n1676.993000\n1887.649500\n2573.170600\n3345.930000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nmens\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n28734.000000\n210.109902\n227.636658\n29.990000\n29.990000\n44.430000\n70.778000\n100.552000\n135.980000\n178.228000\n235.951000\n328.370000\n483.160000\n636.494000\n1119.158900\n3040.200000\n\n\n1\n35266.000000\n268.138832\n274.499729\n29.990000\n29.990000\n55.960000\n89.875000\n131.890000\n183.195000\n240.380000\n315.625000\n424.140000\n623.605000\n810.885000\n1281.637500\n3345.930000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nwomens\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n28818.000000\n209.626353\n224.464931\n29.990000\n29.990000\n44.850000\n70.660000\n100.028000\n135.845000\n177.472000\n235.657000\n331.220000\n485.368000\n634.806000\n1084.233000\n2809.790000\n\n\n1\n35182.000000\n268.673462\n276.633968\n29.990000\n29.990000\n55.790000\n90.363000\n131.912000\n183.565000\n240.728000\n314.427000\n425.060000\n623.798000\n811.720000\n1299.025900\n3345.930000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nzip_code\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nRural\n9563.000000\n242.853935\n253.395958\n29.990000\n29.990000\n49.842000\n80.620000\n117.644000\n159.580000\n212.534000\n281.648000\n386.118000\n563.206000\n742.712000\n1220.397200\n2080.780000\n\n\nSurburban\n28776.000000\n240.488905\n255.448355\n29.990000\n29.990000\n49.810000\n79.130000\n113.440000\n155.520000\n208.120000\n279.100000\n379.150000\n557.290000\n748.607500\n1216.895000\n3345.930000\n\n\nUrban\n25661.000000\n243.589924\n257.971051\n29.990000\n29.990000\n51.060000\n81.260000\n116.280000\n160.280000\n213.170000\n283.440000\n383.070000\n564.210000\n748.330000\n1220.212000\n3215.970000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nnewbie\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n31856.000000\n184.633287\n133.924944\n29.990000\n29.990000\n49.590000\n80.235000\n114.990000\n156.485000\n199.620000\n252.505000\n317.750000\n392.220000\n441.142500\n486.709000\n499.950000\n\n\n1\n32144.000000\n299.023269\n326.128959\n29.990000\n29.990000\n50.966000\n80.120000\n115.670000\n160.555000\n228.358000\n350.740000\n559.612000\n746.218000\n940.859500\n1423.882900\n3345.930000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nchannel\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nMultichannel\n7762.000000\n520.970370\n294.379425\n200.020000\n235.880000\n281.514000\n335.776000\n401.694000\n450.110000\n506.990000\n587.450000\n692.474000\n896.194000\n1099.478000\n1536.610100\n3215.970000\n\n\nPhone\n28021.000000\n202.807184\n223.227547\n29.990000\n29.990000\n43.410000\n68.700000\n97.330000\n131.390000\n170.660000\n224.620000\n309.080000\n462.510000\n642.180000\n1084.122000\n3345.930000\n\n\nWeb\n28217.000000\n204.375017\n226.188279\n29.990000\n29.990000\n43.640000\n68.888000\n97.554000\n131.680000\n171.912000\n225.142000\n312.316000\n466.628000\n646.642000\n1121.830800\n3040.200000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nsegment\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\nMens E-Mail\n21307.000000\n242.835931\n260.355685\n29.990000\n29.990000\n49.692000\n79.358000\n113.878000\n157.220000\n210.652000\n279.066000\n381.840000\n563.628000\n761.734000\n1229.957600\n3215.970000\n\n\nNo E-Mail\n21306.000000\n240.882653\n252.739362\n29.990000\n29.990000\n50.220000\n80.845000\n115.760000\n156.655000\n209.840000\n280.665000\n380.510000\n561.055000\n742.132500\n1191.894500\n3345.930000\n\n\nWomens E-Mail\n21387.000000\n242.536633\n255.332880\n29.990000\n29.990000\n51.172000\n80.374000\n116.396000\n160.090000\n212.182000\n283.610000\n383.848000\n558.162000\n738.333000\n1239.727600\n3040.200000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nvisit\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n54606.000000\n235.163403\n251.053967\n29.990000\n29.990000\n48.720000\n77.450000\n111.150000\n151.835000\n202.310000\n269.785000\n369.440000\n548.045000\n730.865000\n1199.779500\n3345.930000\n\n\n1\n9394.000000\n282.323739\n280.674183\n29.990000\n29.990000\n61.620000\n99.223000\n147.644000\n202.460000\n262.234000\n337.970000\n438.700000\n639.621000\n837.872500\n1335.486700\n3215.970000\n\n\n\n\n\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n95%\n99%\nmax\n\n\nconversion\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n0\n63422.000000\n241.366593\n255.347564\n29.990000\n29.990000\n50.184000\n80.010000\n115.034000\n157.615000\n210.090000\n280.040000\n380.842000\n559.061000\n745.248000\n1215.995800\n3345.930000\n\n\n1\n578.000000\n320.985952\n324.093830\n29.990000\n30.855000\n68.674000\n106.296000\n170.880000\n232.680000\n288.264000\n377.952000\n498.046000\n737.550000\n976.021000\n1634.258900\n2141.120000"
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#treatment-assignment",
    "href": "posts/hillstrom-challenge/meta_models.html#treatment-assignment",
    "title": "About the data",
    "section": "Treatment assignment",
    "text": "Treatment assignment\n\nis the assignment truly random?\n\ntrain logistic regression model to predict the assignment\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\ndf\n\n\n\n\n\n\n\n\nrecency\nhistory_segment\nhistory\nmens\nwomens\nzip_code\nnewbie\nchannel\nsegment\nvisit\nconversion\nspend\n\n\n\n\n0\n10\n2) $100 - $200\n142.44\n1\n0\nSurburban\n0\nPhone\nWomens E-Mail\n0\n0\n0.0\n\n\n1\n6\n3) $200 - $350\n329.08\n1\n1\nRural\n1\nWeb\nNo E-Mail\n0\n0\n0.0\n\n\n2\n7\n2) $100 - $200\n180.65\n0\n1\nSurburban\n1\nWeb\nWomens E-Mail\n0\n0\n0.0\n\n\n3\n9\n5) $500 - $750\n675.83\n1\n0\nRural\n1\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n4\n2\n1) $0 - $100\n45.34\n1\n0\nUrban\n0\nWeb\nWomens E-Mail\n0\n0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63995\n10\n2) $100 - $200\n105.54\n1\n0\nUrban\n0\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n63996\n5\n1) $0 - $100\n38.91\n0\n1\nUrban\n1\nPhone\nMens E-Mail\n0\n0\n0.0\n\n\n63997\n6\n1) $0 - $100\n29.99\n1\n0\nUrban\n1\nPhone\nMens E-Mail\n0\n0\n0.0\n\n\n63998\n1\n5) $500 - $750\n552.94\n1\n0\nSurburban\n1\nMultichannel\nWomens E-Mail\n0\n0\n0.0\n\n\n63999\n1\n4) $350 - $500\n472.82\n0\n1\nSurburban\n0\nWeb\nMens E-Mail\n0\n0\n0.0\n\n\n\n\n64000 rows × 12 columns\n\n\n\n\ndf['segment'].value_counts()\n\nWomens E-Mail    21387\nMens E-Mail      21307\nNo E-Mail        21306\nName: segment, dtype: int64\n\n\n\nX = df[['recency','history','mens', 'womens',  'newbie']]\ny = df['segment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, )\n\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred = lr.predict(X_test)\n\n\ny_pred\n\narray(['Womens E-Mail', 'Mens E-Mail', 'Womens E-Mail', ...,\n       'Mens E-Mail', 'Womens E-Mail', 'Womens E-Mail'], dtype=object)\n\n\n\nprint(classification_report(y_pred=y_pred, y_true=y_test))\n\n               precision    recall  f1-score   support\n\n  Mens E-Mail       0.33      0.21      0.26      5399\n    No E-Mail       0.00      0.00      0.00      5306\nWomens E-Mail       0.33      0.78      0.46      5295\n\n     accuracy                           0.33     16000\n    macro avg       0.22      0.33      0.24     16000\n weighted avg       0.22      0.33      0.24     16000\n\n\n\n/Users/juliapocciotti/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/juliapocciotti/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/juliapocciotti/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#which-email-performed-the-best",
    "href": "posts/hillstrom-challenge/meta_models.html#which-email-performed-the-best",
    "title": "About the data",
    "section": "Which email performed the best",
    "text": "Which email performed the best\n\n(df.groupby(['segment'])\n    [['visit', 'conversion', 'spend']].mean()\n    .style.background_gradient()\n)\n\n\n\n\n\n\n \nvisit\nconversion\nspend\n\n\nsegment\n \n \n \n\n\n\n\nMens E-Mail\n0.182757\n0.012531\n1.422617\n\n\nNo E-Mail\n0.106167\n0.005726\n0.652789\n\n\nWomens E-Mail\n0.151400\n0.008837\n1.077202\n\n\n\n\n\n\n(df.groupby(['segment'])\n    [['visit', 'conversion', 'spend']].describe()\n    .T\n    .style.background_gradient(axis = 1)\n)\n\n\n\n\n\n\n \nsegment\nMens E-Mail\nNo E-Mail\nWomens E-Mail\n\n\n\n\nvisit\ncount\n21307.000000\n21306.000000\n21387.000000\n\n\nmean\n0.182757\n0.106167\n0.151400\n\n\nstd\n0.386476\n0.308059\n0.358447\n\n\nmin\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n\n\nconversion\ncount\n21307.000000\n21306.000000\n21387.000000\n\n\nmean\n0.012531\n0.005726\n0.008837\n\n\nstd\n0.111241\n0.075456\n0.093592\n\n\nmin\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n\n\nspend\ncount\n21307.000000\n21306.000000\n21387.000000\n\n\nmean\n1.422617\n0.652789\n1.077202\n\n\nstd\n17.754205\n11.588200\n15.116106\n\n\nmin\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n\n\nmax\n499.000000\n499.000000\n499.000000"
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#if-you-could-only-send-an-e-mail-campaign-to-the-best-10000-customers-which-customers-would-receive-the-e-mail-campaign-why",
    "href": "posts/hillstrom-challenge/meta_models.html#if-you-could-only-send-an-e-mail-campaign-to-the-best-10000-customers-which-customers-would-receive-the-e-mail-campaign-why",
    "title": "About the data",
    "section": "If you could only send an e-mail campaign to the best 10,000 customers, which customers would receive the e-mail campaign? Why?",
    "text": "If you could only send an e-mail campaign to the best 10,000 customers, which customers would receive the e-mail campaign? Why?\n\nWhich customers would benefit the most from the communication and which ones would already buy anyway?\n\n\nCategorical outcomes\n\nT-learner\n\nPick the observation of interest.\nFix the treatment value for this observation to 1 (or True).\nPredict the outcome using the trained model.\nTake the same observation again.\nThis time, fix the value of the treatment to 0 (or False).\nGenerate the prediction.\nSubtract the value of the prediction without treatment from the value of the prediction with treatment.\n\n\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\nSplit the problem for Men’s and Women’s emails\n\ntype_schema = {\n    'history_segment': 'category',\n    'zip_code': 'category',\n    'channel': 'category'\n}\n\n\nmen = df.query('segment != \"Womens E-Mail\"').astype(type_schema).reset_index(drop = True)\nwomen = df.query('segment != \"Mens E-Mail\"').astype(type_schema).reset_index(drop = True)\n\nmen = men.assign(\n    segment = lambda d: d.segment.replace({'Mens E-Mail':1, 'No E-Mail': 0})\n)\n\nwomen = women.assign(\n    segment = lambda d: d.segment.replace({'Womens E-Mail':1, 'No E-Mail': 0})\n)\n\n\ncols = ['recency', 'history_segment', 'history', 'mens', 'womens', 'zip_code', 'newbie', 'channel', 'segment', 'spend']\nX = ['recency', 'history_segment', 'history', 'mens', 'womens', 'zip_code', 'newbie', 'channel']\n# X = ['recency', 'history', 'mens', 'womens', 'newbie']\n\ny = 'spend'\nT = 'segment'\n\ntrain_men, test_men = train_test_split(men[cols], random_state=123)\ntrain_women, test_women = train_test_split(women[cols], random_state=123)\n\n\nm0 = LGBMRegressor(verbose = -1)\nm1 = LGBMRegressor(verbose = -1)\n\nm0.fit(train_women.query(f\"segment==0\")[X], train_women.query(f\"segment==0\")[y])\nm1.fit(train_women.query(f\"segment==1\")[X], train_women.query(f\"segment==1\")[y])\n\nt_learner_cate_test = test_women.assign(\n            cate=m1.predict(test_women[X]) - m0.predict(test_women[X]),\n            cate_bins = lambda d: pd.qcut(d['cate'], q = 5), \n        )\n\nt_learner_cate_test.groupby('cate_bins')['spend'].mean()\n\ncate_bins\n(-59.513999999999996, -0.609]    1.182009\n(-0.609, 0.00947]                1.182190\n(0.00947, 0.28]                  0.695904\n(0.28, 1.237]                    0.781790\n(1.237, 36.935]                  1.378848\nName: spend, dtype: float64\n\n\n\nuntreated = (t_learner_cate_test\n    .query('segment == 0')\n    .groupby('cate_bins')\n    .spend.mean()\n    .sort_index()\n)\n\n\ntreated = (t_learner_cate_test\n    .query('segment == 1')\n    .groupby('cate_bins')\n    .spend.mean()\n    .sort_index()\n)\n\n\n(treated - untreated).plot.bar()\n\n&lt;Axes: xlabel='cate_bins'&gt;\n\n\n\n\n\n\ngain_curve_test = relative_cumulative_gain_curve(t_learner_cate_test, 'segment', y, prediction=\"cate\")\nauc = area_under_the_relative_cumulative_gain_curve(t_learner_cate_test, 'segment', y, prediction=\"cate\")\n\nplt.figure(figsize=(10,4))\nplt.plot(gain_curve_test, color=\"C0\", label=f\"AUC: {auc:.2f}\")\nplt.hlines(0, 0, 100, linestyle=\"--\", color=\"black\", label=\"Baseline\")\n\nplt.legend();\nplt.title(\"T-Learner\")\n\nText(0.5, 1.0, 'T-Learner')"
  },
  {
    "objectID": "posts/hillstrom-challenge/meta_models.html#x-learner",
    "href": "posts/hillstrom-challenge/meta_models.html#x-learner",
    "title": "About the data",
    "section": "X-Learner",
    "text": "X-Learner\nSteps\n\nSame as T-Learner\n\nWe split our data on the treatment variable so that we obtain two separate subsets: the first containing treated units only, and the second one containing untreated units only. Next, we train two models: one on each subset. We call these models μ 1 and μ 0, respectively.\n\nCompute its individual estimated treatment effect\n\nGet imputed scores: y_pred_treated - y_true_treated\ny_pred_untreated - y_true_untreated\nTrain two more models to learn to predict the imputed scores based on the feature vectors\nGet propensity scores → probability of receiving the treatment\nCATE = prob_1 * imputed_score_pred_0 + prob_0 * imputed_score_pred_1\n\n\n\nnp.random.seed(123)\n\n# first stage models\nm0 = LGBMRegressor(verbose = -1)\nm1 = LGBMRegressor(verbose = -1)\n\nm0.fit(train_women.query(f\"segment==0\")[X], train_women.query(f\"segment==0\")[y])\nm1.fit(train_women.query(f\"segment==1\")[X], train_women.query(f\"segment==1\")[y])\n\nLGBMRegressor(verbose=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMRegressorLGBMRegressor(verbose=-1)\n\n\n\n# propensity score model\ng = LogisticRegression(solver=\"lbfgs\", penalty='none') \n\ng_data = pd.concat([train_women.drop(list(type_schema.keys()) + ['segment'] + ['spend'], axis = 1), \n                    pd.get_dummies(train_women[type_schema.keys()])\n                ], \n               axis = 1\n               )\n\n\ng.fit(g_data, train_women[T])\n\n/Users/juliapocciotti/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none')\n\n\n\nd_train = np.where(train_women[T]==0,\n                   m1.predict(train_women[X]) - train_women[y],\n                   train_women[y] - m0.predict(train_women[X]))\n\n# second stage\nmx0 = LGBMRegressor(max_depth=2, min_child_samples=30)\nmx1 = LGBMRegressor(max_depth=2, min_child_samples=30)\n\nmx0.fit(train_women.query(f\"{T}==0\")[X], d_train[train_women[T]==0])\nmx1.fit(train_women.query(f\"{T}==1\")[X], d_train[train_women[T]==1]);\n\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 290\n[LightGBM] [Info] Number of data points in the train set: 15965, number of used features: 8\n[LightGBM] [Info] Start training from score 0.427583\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 290\n[LightGBM] [Info] Number of data points in the train set: 16054, number of used features: 8\n[LightGBM] [Info] Start training from score 0.361798\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n\ndef ps_predict(df, t): \n    df = pd.concat([df.drop(list(type_schema.keys()) + ['segment'] + ['spend'], axis = 1), \n                    pd.get_dummies(df[type_schema.keys()])\n                ], \n               axis = 1\n        )\n    \n    return g.predict_proba(df)[:, t]\n    \n    \nx_cate_train = (ps_predict(train_women,1)*mx0.predict(train_women[X]) +\n                ps_predict(train_women,0)*mx1.predict(train_women[X]))\n\nx_cate_test = test_women.assign(cate=(ps_predict(test_women,1)*mx0.predict(test_women[X]) +\n                                ps_predict(test_women,0)*mx1.predict(test_women[X])))\n\n\ngain_curve_test = relative_cumulative_gain_curve(x_cate_test, 'segment', y, prediction=\"cate\")\nauc = area_under_the_relative_cumulative_gain_curve(x_cate_test, 'segment', y, prediction=\"cate\")\n\nplt.figure(figsize=(10,4))\nplt.plot(gain_curve_test, color=\"C0\", label=f\"AUC: {auc:.2f}\")\nplt.hlines(0, 0, 100, linestyle=\"--\", color=\"black\", label=\"Baseline\")\n\nplt.legend();\nplt.title(\"X-Learner\")\n\nText(0.5, 1.0, 'X-Learner')"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html",
    "href": "posts/probability-calibration/prob_calibration.html",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "We can say that a well calibrated classifier is one that its probabilities corresponds to the real fraction of positives we observe in real life, meaning that we could interpret its probabilities as a confidence level as well. An example of a well calibrated (binary) classifier is one that if in 100 examples it predicts a probability of 0.7, then 70% of the examples will have class 1 and 30% will have a class 0.\n\n\n\nIf we have a problem that we only care about the predictions of class 1 or 0, then we don’t need to care for calibration, but if it is the case that the probabilities of the model will be used in our real life situation, then we would want that the probabilities really represents the likelihood of these events.\n\n\n\nLet’s use an example of fraud detection, we can train a ML model to predict if a transaction is fraudulent or not (1/0). But for fraud detection, it is often the case that we need to analyze the costs involved in the prediction to determine if a fraud is likely fraudulent or not. A framework that can be used is the following:\n\nSuppose we get a 10% comission over the amount transactioned if the transaction is complete, on the other hand, if the transacition was actually a fraud, then we would have to return to the customer 100% of the amount paid. In this case, a False Negative (FN) means that we would have a cost of -total amount, and a True Negative, a profit of 10% * total amount\nIn this scenario, there are different costs associated with each prediction, so we could classify the examples based on the expected revenue for class 1, and the expected revenue for class 0. In the end, we can choose the prediction that has the highest expected revenue:\n\\[\\textnormal{expected revenue 1}  = (profitTP * prob1) + (costFP * prob0)\\] \\[\\textnormal{expected revenue 0} = (profitTN * prob0) + (costFN * prob1)\\]\nFor each value in the confusion matrix, we can assign the following profit/cost structure:\n\nTP: 0 cost or profit because we classify the fraudulent transaction as fraud\nTN: 10% comission over the total amount\nFP: -10% comission, beacause we classify a fraud as one when it isn’t\nFN: -total amount, real fraud that wasn’t captured by the model -&gt; we have to return the total amount to the customer\n\nLet’s use as an example a transaction of $50 that is not fraudulent and assume our model predicts a probability of 0.4 of being fraudulent, when the real probability was 0.1. Here is the impact of this difference:\n\n\ndef get_example_value_dependent(prob_1, total_amount): \n    prob_0 = 1 - prob_1\n\n    rev_tp = 0\n    rev_fp = - (total_amount * 0.1)\n    rev_tn = total_amount * 0.1\n    rev_fn = -total_amount\n\n    expected_rev_1 =  (rev_tp * prob_1) + (rev_fp * prob_0)\n    expected_rev_0 =  (rev_tn * prob_0) + (rev_fn * prob_1)\n\n    return expected_rev_1, expected_rev_0\n\ndef classify_sample_revenue(expected_rev_1, expected_rev_0): \n    if expected_rev_1 &gt; expected_rev_0: return 1\n    else: return 0\n\n\nmodel_probability = 0.4\nexpected_rev_1, expected_rev_0 = get_example_value_dependent(model_probability, total_amount = 50)\nprint(f'Expected revenue class 1: {expected_rev_1}')\nprint(f'Expected revenue class 0: {expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample_revenue(expected_rev_1, expected_rev_0)}\\n')\n\nreal_probability = 0.1 \nreal_expected_rev_1, real_expected_rev_0 = get_example_value_dependent(real_probability, total_amount = 50)\nprint(f'Real expected revenue class 1: {real_expected_rev_1}')\nprint(f'Real expected revenue class 0: {real_expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample_revenue(real_expected_rev_1, real_expected_rev_0)}')\n\nExpected revenue class 1: -3.0\nExpected revenue class 0: -17.0\nClassification by model probability: 1\n\nReal expected revenue class 1: -4.5\nReal expected revenue class 0: -0.5\nClassification by model probability: 0\n\n\n\n\n\n\nAlgorithms not trained using a probabilistic framework\nThere are only a few models that produce calibrated probabilities. That is because in order to give calibrated probabilities, the model must be trained in a probabilistic framework, such as maximum likelihood estimation. The main example that can return probabilities already calibrated is a Logistic Regression. ANNs can also have better calibration of probabilities than other models.\nThe opposite can occur for methods such as in Bagging Estimators. For a Random Forest to predict a probability = 0, it means that all estimators in the forest predict 0. In a Random Forest this is harder to occur because of the feature subsample used to build each tree, meaning that there is a relative high variance between the trees in the forest.\nClass imbalance\nWhen there is a case of high class imbalance, such as in fraud prevention problems, the model will naturally predict higher probabilities for the majority class.\n\n\n\n\nFor the rest of this post, we’ll use the Credit Card Fraud Detection dataset available on Kaggle. Our goal will be to train a model to predict fraud and then use the same framework as before to classify each example as a fraud or not considering the expected profits and costs with each classification\n\ndf = pd.read_csv('creditcard.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n0.090794\n-0.551600\n-0.617801\n-0.991390\n-0.311169\n1.468177\n-0.470401\n0.207971\n0.025791\n0.403993\n0.251412\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n-0.166974\n1.612727\n1.065235\n0.489095\n-0.143772\n0.635558\n0.463917\n-0.114805\n-0.183361\n-0.145783\n-0.069083\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n0.207643\n0.624501\n0.066084\n0.717293\n-0.165946\n2.345865\n-2.890083\n1.109969\n-0.121359\n-2.261857\n0.524980\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n-0.054952\n-0.226487\n0.178228\n0.507757\n-0.287924\n-0.631418\n-1.059647\n-0.684093\n1.965775\n-1.232622\n-0.208038\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n0.753074\n-0.822843\n0.538196\n1.345852\n-1.119670\n0.175121\n-0.451449\n-0.237033\n-0.038195\n0.803487\n0.408542\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n\n\n\nAs expected for fraud prediction problems, we can see there is a high class imbalance between the classes:\n\ndf['Class'].value_counts(normalize = True)\n\n0    0.998273\n1    0.001727\nName: Class, dtype: float64\n\n\nLet’s perform an undersample of the majority class to get only 100.000 samples from class 0 and keep all samples from class 1:\n\nclass_0 = df.query('Class == 0').sample(100_000)\nclass_1 = df.query('Class == 1')\n\nfraud_data = pd.concat([class_0, class_1], axis = 0).reset_index(drop = True)\n\n\nfraud_data.Class.value_counts(normalize = True)\n\n0    0.995104\n1    0.004896\nName: Class, dtype: float64\n\n\nThere is still a very high class imbalance, but now it will be a bit easier to fit the model.\nNext, let’s split the data in a train and test sets and train an initial model:\n\nX = fraud_data.drop(['Class'], axis = 1)\ny = fraud_data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X_train, y_train);\n\n\nmodel_preds = model.predict(X_test)\n\nprint(classification_report(y_test, model_preds))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     25006\n           1       0.93      0.80      0.86       117\n\n    accuracy                           1.00     25123\n   macro avg       0.96      0.90      0.93     25123\nweighted avg       1.00      1.00      1.00     25123\n\n\n\nGreat! The model seems to present a nice performance on the test. Now, let’s move to the calibration of probabilities.\n\n\n\n\n\nTo better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn, this can be easily done with sklearn.calibration.calibration_curve. Even though scikit-learn has a built in class to plot the calibration curve, I personally prefer plotly as a library for visualization. With that in mind, we can use the following function to plot the calibration curve:\n\nfrom sklearn.calibration import calibration_curve\n\ndef plot_calibration_curve(models_probabilities, n_bins = 5, title = 'Calibration Curve'):\n\n    fig = sp.make_subplots(rows=1, cols=1)\n\n    for model_name, model_probs in list(models_probabilities.items()):\n        prob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=n_bins)\n\n        fig.add_trace(\n            go.Scatter(x=prob_pred, y=prob_true, mode='lines+markers', name=model_name),\n            row=1, col=1\n        )\n\n    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), name='Perfectly Calibrated'), row=1, col=1)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Mean Predicted Probability\",\n        yaxis_title=\"Fraction of Positives\",\n        showlegend=True, \n        autosize = False, \n        width = 800, height = 500\n    )\n    \n    fig.show()\n    \n\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Prepare input \nmodels_probabilities = {'Extra Trees':model_probs}\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 10, title = 'Extra Trees calibration curve')\n\n\n                                                \n\n\nThe number of bins can be configured to calculate the fraction of positives. If we have to many of them, each bin will have few points and it will be difficult to understand the calibration, since the graph will have a lot of noise. And the opposite will occur if we have too few of them. Here is an example of this difference:\n\nplot_calibration_curve(models_probabilities, n_bins = 20, title = 'Calibration Curve with 20 bins')\n\n\n                                                \n\n\n\nplot_calibration_curve(models_probabilities, n_bins = 3, title = 'Calibration Curve with 3 bins')\n\n\n                                                \n\n\nOverall, I find that somewhere between 5 and 10 is good place to select the number of bins\nLet’s also train a few more models so we can compare them:\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\n# LGBM, param grid optimized with optuna to maximize recall\nparam_grid_lgbm = {'n_estimators': 392,\n                    'bagging_freq':1,\n                    'learning_rate': 0.010990720231827398,\n                    'num_leaves': 14,\n                    'subsample': 0.8558243397218831,\n                    'colsample_bytree': 0.8486401096993541,\n                    'min_data_in_leaf': 53}\n                    \nlgbm = LGBMClassifier(verbose = -1, \n    class_weight = class_weight, \n    **param_grid_lgbm\n)\nlgbm.fit(X_train, y_train)\n\n# KNN, param grid optimized with optuna to maximize recall\nparam_grid_knn = {'n_neighbors': 5,\n                    'weights': 'uniform',\n                    'algorithm': 'kd_tree',\n                    'metric': 'manhattan'}\nknn = KNeighborsClassifier(**param_grid_knn)\nknn.fit(X_train, y_train)\n\n\n# Extra Trees, param grid optimized with optuna to maximize recall\nparam_grid_et = {\n  'n_estimators': 449,\n  'max_depth': 50,\n  'max_features': 'sqrt',\n  'min_samples_leaf': 68, \n  'class_weight':class_weight, \n  'bootstrap': True\n}\net = ExtraTreesClassifier(**param_grid_et)\net.fit(X_train, y_train);\n\n\n# Prepare input \nmodels_probabilities = {'Extra Trees'        : et.predict_proba(X_test)[:, 1], \n                        'Naive Bayes'        : nb.predict_proba(X_test)[:, 1], \n                        'LGBM'               : lgbm.predict_proba(X_test)[:, 1], \n                        'KNN'                : knn.predict_proba(X_test.values)[:, 1], \n                    }\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 5)\n\n\n                                                \n\n\n\n\n\nThe Brier Score simply calculates the mean squared difference between the model’s probabilities and the actual outcome. The score ranges from 0 to 1, and the lower the better\n\\[Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - p_i)^2\\]\nSo now let’s calculate the brier score with the original probabilities from the models:\n\nfrom sklearn.metrics import brier_score_loss\n\nfor model_name, prob in list(models_probabilities.items()):\n    print(f\"{model_name} brier score: {brier_score_loss(y_test, prob)}\")\n\nExtra Trees brier score: 0.04290825440034065\nNaive Bayes brier score: 0.008200641198818524\nLGBM brier score: 0.005798401913249227\nKNN brier score: 0.0036651673765075825\n\n\n\n\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, in which the process is repeated over k times.\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities, available as the parameter method = 'sigmoid' in the CalibratedClassifierCV class.\nThe other way is to an Isotonic Regression, that functions as a non-parametrical approach since it doesn’t make any assumptions about how the data is distributed. Using an isotonic regression is more powerful, but it also requires more training data. Scikit-learn’s documentation recommends to have at least 1000 samples in each fold. In our case we have over 75k samples for training, so that won’t be an issue. In this example we’ll use a isotonic regression through the parameter method = 'isotonic' in the CalibratedClassifierCV class.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nclf_list = [\n    (knn, \"KNN\"),\n    (nb, \"Naive Bayes\"),\n    (lgbm, \"LGBM\"),\n    (et, \"Extra Trees\"),\n]\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5, method = 'isotonic')\n    calibrated_clf.fit(X_train.values, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\nWe can plot again the new calibration curve:\n\nmodels_probabilities_calibrated = {model_name: model_calibrated.predict_proba(X_test.values)[:, 1] \\\n                                              for model_name, model_calibrated in calibrated_clfs.items()}\n\nplot_calibration_curve(models_probabilities_calibrated, n_bins = 5)\n\n\n                                                \n\n\n\n\n\n\n\nNow that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\n# Create a new dataframe with the calibrated probabilities\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_et = calibrated_clfs['Extra Trees'].predict_proba(X_test)[:, 1], \n    probs_knn = calibrated_clfs['KNN'].predict_proba(X_test.values)[:, 1],\n    y_true = y_test\n)\n\nWe can use the same functions as before to calculate the expected revenue for each class and then calculate what class the sample will be based on its revenue.\nAfter the predictions are made, we can access the real revenue by classifying the prediction as a TP/TN/FN/FP and calculating the costs/profits generated by a TP/TN/FN/FP classification. We can calculate this with the following functions:\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\nNow, we just need to calculate the revenues from the calibrated models:\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_et', 'probs_knn']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n    \n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n\n        # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n\n        # Gets the identification if the prediction is a TP/TN/FN/FP\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n\n        # Returns the real revenue by the prediction\n        real_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.real_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.real_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  219393.72\n\n probs_nb expected revenue:  186849.94\n\n probs_et expected revenue:  218477.48\n\n probs_knn expected revenue:  209727.42\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\n\n\n\n\n\n\n\n\nrevenues_not_calibrated\nrevenues_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n178397.34\n219393.72\n40996.38\n\n\nprobs_nb\n182901.17\n186849.94\n3948.77\n\n\nprobs_et\n219313.71\n218477.48\n-836.23\n\n\nprobs_knn\n203350.95\n209727.42\n6376.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nold_brier_score\nnew_brier_score\nbrier_score_diff\n\n\n\n\nprobs_lgbm\n0.0058\n0.00132\n-0.00448\n\n\nprobs_nb\n0.0082\n0.00343\n-0.00477\n\n\nprobs_et\n0.00107\n0.00151\n0.00044\n\n\nprobs_knn\n0.00367\n0.00351\n-0.00016\n\n\n\n\n\n\n\nIn general, we could improve the total revenue results with the calibration of probabilities. We can verify that the models with the highest difference in the brier score were also the ones with the highest increase in revenue. For models that had good calibration originally, such as the Extra Trees classifier, the process didn’t increase revenue.\nFinally, we can calculate what is the best model based on the total revenue generated by it and how much we could improve it with the calibration of probabilities:\n\nbest_model   = compare_revs.revenues_calibrated.idxmax()\n\nrevenue      = compare_revs.loc[best_model].revenues_calibrated.round(2)\nrevenue_diff = compare_revs.loc[best_model].revenue_diff.round(2)\n\nprint(f'''\nBest model probabilities are from {best_model}, with a total revenue of {revenue}. \nThe revenue increase after calibrating probabilities was a total of {revenue_diff}\n ''')\n\n\nBest model probabilities are from probs_lgbm, with a total revenue of 219393.72. \nThe revenue increase after calibrating probabilities was a total of 40996.38\n \n\n\n\n\n\nCalibrating probabilities can be useful in a lot a different scenarios when we are working with the real probabilities and not only caring for classifing examples as 1/0. I believe that the cost classification model can be very useful since its metrics are the real profits and costs involved in the model’s predictions, and for the business, in the end, that’s really what matters. Working with calibrating probabilities in this specific framework can have a real increase in these metrics.\nEven though there were a lot concepts here, we are only scratching the surface in these topics. I recently discovered this framework of “Cost-sensitive learning” and found there is a range of different techniques in it. For anyone interested in this topic, I recommend the paper The Foundations of Cost-Sensitive Learning, by Charles Elkan.\nWhen it comes to the calibration of probabilities, there is also many more techniques that could be applied to improve calibration. For a more technical analysis on the subject, I recommend the chapter of Probability Calibration from the book Imbalanced Binary Classification: A Survey with code, where the authors explore different calibration techniques and focus on how to apply calibration in imbalanced problems, such as the one we worked with here."
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "href": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "To better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn, this can be easily done with sklearn.calibration.calibration_curve. Even though scikit-learn has a built in class to plot the calibration curve, I personally prefer plotly as a library for visualization. With that in mind, we can use the following function to plot the calibration curve:\n\nfrom sklearn.calibration import calibration_curve\n\ndef plot_calibration_curve(models_probabilities, n_bins = 5, title = 'Calibration Curve'):\n\n    fig = sp.make_subplots(rows=1, cols=1)\n\n    for model_name, model_probs in list(models_probabilities.items()):\n        prob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=n_bins)\n\n        fig.add_trace(\n            go.Scatter(x=prob_pred, y=prob_true, mode='lines+markers', name=model_name),\n            row=1, col=1\n        )\n\n    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), name='Perfectly Calibrated'), row=1, col=1)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Mean Predicted Probability\",\n        yaxis_title=\"Fraction of Positives\",\n        showlegend=True, \n        autosize = False, \n        width = 800, height = 500\n    )\n    \n    fig.show()\n    \n\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Prepare input \nmodels_probabilities = {'Extra Trees':model_probs}\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 10, title = 'Extra Trees calibration curve')\n\n\n                                                \n\n\nThe number of bins can be configured to calculate the fraction of positives. If we have to many of them, each bin will have few points and it will be difficult to understand the calibration, since the graph will have a lot of noise. And the opposite will occur if we have too few of them. Here is an example of this difference:\n\nplot_calibration_curve(models_probabilities, n_bins = 20, title = 'Calibration Curve with 20 bins')\n\n\n                                                \n\n\n\nplot_calibration_curve(models_probabilities, n_bins = 3, title = 'Calibration Curve with 3 bins')\n\n\n                                                \n\n\nOverall, I find that somewhere between 5 and 10 is good place to select the number of bins\nLet’s also train a few more models so we can compare them:\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\n# LGBM, param grid optimized with optuna to maximize recall\nparam_grid_lgbm = {'n_estimators': 392,\n                    'bagging_freq':1,\n                    'learning_rate': 0.010990720231827398,\n                    'num_leaves': 14,\n                    'subsample': 0.8558243397218831,\n                    'colsample_bytree': 0.8486401096993541,\n                    'min_data_in_leaf': 53}\n                    \nlgbm = LGBMClassifier(verbose = -1, \n    class_weight = class_weight, \n    **param_grid_lgbm\n)\nlgbm.fit(X_train, y_train)\n\n# KNN, param grid optimized with optuna to maximize recall\nparam_grid_knn = {'n_neighbors': 5,\n                    'weights': 'uniform',\n                    'algorithm': 'kd_tree',\n                    'metric': 'manhattan'}\nknn = KNeighborsClassifier(**param_grid_knn)\nknn.fit(X_train, y_train)\n\n\n# Extra Trees, param grid optimized with optuna to maximize recall\nparam_grid_et = {\n  'n_estimators': 449,\n  'max_depth': 50,\n  'max_features': 'sqrt',\n  'min_samples_leaf': 68, \n  'class_weight':class_weight, \n  'bootstrap': True\n}\net = ExtraTreesClassifier(**param_grid_et)\net.fit(X_train, y_train);\n\n\n# Prepare input \nmodels_probabilities = {'Extra Trees'        : et.predict_proba(X_test)[:, 1], \n                        'Naive Bayes'        : nb.predict_proba(X_test)[:, 1], \n                        'LGBM'               : lgbm.predict_proba(X_test)[:, 1], \n                        'KNN'                : knn.predict_proba(X_test.values)[:, 1], \n                    }\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 5)\n\n\n                                                \n\n\n\n\n\nThe Brier Score simply calculates the mean squared difference between the model’s probabilities and the actual outcome. The score ranges from 0 to 1, and the lower the better\n\\[Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - p_i)^2\\]\nSo now let’s calculate the brier score with the original probabilities from the models:\n\nfrom sklearn.metrics import brier_score_loss\n\nfor model_name, prob in list(models_probabilities.items()):\n    print(f\"{model_name} brier score: {brier_score_loss(y_test, prob)}\")\n\nExtra Trees brier score: 0.04290825440034065\nNaive Bayes brier score: 0.008200641198818524\nLGBM brier score: 0.005798401913249227\nKNN brier score: 0.0036651673765075825\n\n\n\n\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, in which the process is repeated over k times.\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities, available as the parameter method = 'sigmoid' in the CalibratedClassifierCV class.\nThe other way is to an Isotonic Regression, that functions as a non-parametrical approach since it doesn’t make any assumptions about how the data is distributed. Using an isotonic regression is more powerful, but it also requires more training data. Scikit-learn’s documentation recommends to have at least 1000 samples in each fold. In our case we have over 75k samples for training, so that won’t be an issue. In this example we’ll use a isotonic regression through the parameter method = 'isotonic' in the CalibratedClassifierCV class.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nclf_list = [\n    (knn, \"KNN\"),\n    (nb, \"Naive Bayes\"),\n    (lgbm, \"LGBM\"),\n    (et, \"Extra Trees\"),\n]\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5, method = 'isotonic')\n    calibrated_clf.fit(X_train.values, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\nWe can plot again the new calibration curve:\n\nmodels_probabilities_calibrated = {model_name: model_calibrated.predict_proba(X_test.values)[:, 1] \\\n                                              for model_name, model_calibrated in calibrated_clfs.items()}\n\nplot_calibration_curve(models_probabilities_calibrated, n_bins = 5)"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "href": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "Now that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\n# Create a new dataframe with the calibrated probabilities\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_et = calibrated_clfs['Extra Trees'].predict_proba(X_test)[:, 1], \n    probs_knn = calibrated_clfs['KNN'].predict_proba(X_test.values)[:, 1],\n    y_true = y_test\n)\n\nWe can use the same functions as before to calculate the expected revenue for each class and then calculate what class the sample will be based on its revenue.\nAfter the predictions are made, we can access the real revenue by classifying the prediction as a TP/TN/FN/FP and calculating the costs/profits generated by a TP/TN/FN/FP classification. We can calculate this with the following functions:\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\nNow, we just need to calculate the revenues from the calibrated models:\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_et', 'probs_knn']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n    \n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n\n        # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n\n        # Gets the identification if the prediction is a TP/TN/FN/FP\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n\n        # Returns the real revenue by the prediction\n        real_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.real_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.real_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  219393.72\n\n probs_nb expected revenue:  186849.94\n\n probs_et expected revenue:  218477.48\n\n probs_knn expected revenue:  209727.42\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\n\n\n\n\n\n\n\n\nrevenues_not_calibrated\nrevenues_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n178397.34\n219393.72\n40996.38\n\n\nprobs_nb\n182901.17\n186849.94\n3948.77\n\n\nprobs_et\n219313.71\n218477.48\n-836.23\n\n\nprobs_knn\n203350.95\n209727.42\n6376.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nold_brier_score\nnew_brier_score\nbrier_score_diff\n\n\n\n\nprobs_lgbm\n0.0058\n0.00132\n-0.00448\n\n\nprobs_nb\n0.0082\n0.00343\n-0.00477\n\n\nprobs_et\n0.00107\n0.00151\n0.00044\n\n\nprobs_knn\n0.00367\n0.00351\n-0.00016\n\n\n\n\n\n\n\nIn general, we could improve the total revenue results with the calibration of probabilities. We can verify that the models with the highest difference in the brier score were also the ones with the highest increase in revenue. For models that had good calibration originally, such as the Extra Trees classifier, the process didn’t increase revenue.\nFinally, we can calculate what is the best model based on the total revenue generated by it and how much we could improve it with the calibration of probabilities:\n\nbest_model   = compare_revs.revenues_calibrated.idxmax()\n\nrevenue      = compare_revs.loc[best_model].revenues_calibrated.round(2)\nrevenue_diff = compare_revs.loc[best_model].revenue_diff.round(2)\n\nprint(f'''\nBest model probabilities are from {best_model}, with a total revenue of {revenue}. \nThe revenue increase after calibrating probabilities was a total of {revenue_diff}\n ''')\n\n\nBest model probabilities are from probs_lgbm, with a total revenue of 219393.72. \nThe revenue increase after calibrating probabilities was a total of 40996.38"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#final-thoughts",
    "href": "posts/probability-calibration/prob_calibration.html#final-thoughts",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "Calibrating probabilities can be useful in a lot a different scenarios when we are working with the real probabilities and not only caring for classifing examples as 1/0. I believe that the cost classification model can be very useful since its metrics are the real profits and costs involved in the model’s predictions, and for the business, in the end, that’s really what matters. Working with calibrating probabilities in this specific framework can have a real increase in these metrics.\nEven though there were a lot concepts here, we are only scratching the surface in these topics. I recently discovered this framework of “Cost-sensitive learning” and found there is a range of different techniques in it. For anyone interested in this topic, I recommend the paper The Foundations of Cost-Sensitive Learning, by Charles Elkan.\nWhen it comes to the calibration of probabilities, there is also many more techniques that could be applied to improve calibration. For a more technical analysis on the subject, I recommend the chapter of Probability Calibration from the book Imbalanced Binary Classification: A Survey with code, where the authors explore different calibration techniques and focus on how to apply calibration in imbalanced problems, such as the one we worked with here."
  },
  {
    "objectID": "course-notes/econometrics-ml/first-lecture/index.html",
    "href": "course-notes/econometrics-ml/first-lecture/index.html",
    "title": "First lecture",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "About the data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEveryday pandas\n\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nJulia Pocciotti\n\n\n\n\n\n\n  \n\n\n\n\nProbability Calibration & Cost classification models\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\nNo matching items"
  }
]