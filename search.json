[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, I’m Julia! I currently work as a Data Scientist in the banking industry.\nMy journey into the world of Data Science began during my college years, and it was during this time that I became a member of Turing USP. This is a student-led group at the University of São Paulo, dedicated to the realm of Artificial Intelligence. When I initially joined the group, I had little to no knowledge about AI, but we had a system that knowing the subjects first hand or not really didn’t matter. The goal was to pick a topic that you found interesting, study it, and then share with the rest of the group. That way of learning really helped me, and to this day I believe that is the best way to learn something new.\nWith that in mind, my ideia with this blog is to make as a way for me to better study topics I’m interested in, share notes on courses I’m taking, and also use it in general as a way to better structure thoughts and experiments I’m currently working on."
  },
  {
    "objectID": "course-notes/index.html",
    "href": "course-notes/index.html",
    "title": "Course notes",
    "section": "",
    "text": "First lecture\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html",
    "href": "posts/probability-calibration/prob_calibration.html",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "We can say that a well calibrated classifier is one that its probabilities corresponds to the real fraction of positives we observe in real life, meaning that we could interpret its probabilities as a confidence level as well. An example of a well calibrated (binary) classifier is one that if in 100 examples it predicts a probability of 0.7, then 70% of the examples will have class 1 and 30% will have a class 0.\n\n\n\nIf we have a problem that we only care about the predictions of class 1 or 0, then we don’t need to care for calibration, but if it is the case that the probabilities of the model will be used in our real life situation, then we would want that the probabilities really represents the likelihood of these events.\n\n\n\nLet’s use an example of fraud detection, we can train a ML model to predict if a transaction is fraudulent or not (1/0). But for fraud detection, it is often the case that we need to analyze the costs involved in the prediction to determine if a fraud is likely fraudulent or not. A framework that can be used is the following:\n\nSuppose we get a 10% comission over the amount transactioned if the transaction is complete, on the other hand, if the transacition was actually a fraud, then we would have to return to the customer 100% of the amount paid. In this case, a False Negative (FN) means that we would have a cost of -total amount, and a True Negative, a profit of 10% * total amount\nIn this scenario, there are different costs associated with each prediction, so we could classify the examples based on the expected revenue for class 1, and the expected revenue for class 0. In the end, we can choose the prediction that has the highest expected revenue:\n\\[\\textnormal{expected revenue 1}  = (profitTP * prob1) + (costFP * prob0)\\] \\[\\textnormal{expected revenue 0} = (profitTN * prob0) + (costFN * prob1)\\]\nFor each value in the confusion matrix, we can assign the following profit/cost structure:\n\nTP: 0 cost or profit because we classify the fraudulent transaction as fraud\nTN: 10% comission over the total amount\nFP: -10% comission, beacause we classify a fraud as one when it isn’t\nFN: -total amount, real fraud that wasn’t captured by the model -&gt; we have to return the total amount to the customer\n\nLet’s use as an example a transaction of $50 that is not fraudulent and assume our model predicts a probability of 0.4 of being fraudulent, when the real probability was 0.1. Here is the impact of this difference:\n\n\ndef get_example_value_dependent(prob_1, total_amount): \n    prob_0 = 1 - prob_1\n\n    rev_tp = 0\n    rev_fp = - (total_amount * 0.1)\n    rev_tn = total_amount * 0.1\n    rev_fn = -total_amount\n\n    expected_rev_1 =  (rev_tp * prob_1) + (rev_fp * prob_0)\n    expected_rev_0 =  (rev_tn * prob_0) + (rev_fn * prob_1)\n\n    return expected_rev_1, expected_rev_0\n\ndef classify_sample_revenue(expected_rev_1, expected_rev_0): \n    if expected_rev_1 &gt; expected_rev_0: return 1\n    else: return 0\n\n\nmodel_probability = 0.4\nexpected_rev_1, expected_rev_0 = get_example_value_dependent(model_probability, total_amount = 50)\nprint(f'Expected revenue class 1: {expected_rev_1}')\nprint(f'Expected revenue class 0: {expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample_revenue(expected_rev_1, expected_rev_0)}\\n')\n\nreal_probability = 0.1 \nreal_expected_rev_1, real_expected_rev_0 = get_example_value_dependent(real_probability, total_amount = 50)\nprint(f'Real expected revenue class 1: {real_expected_rev_1}')\nprint(f'Real expected revenue class 0: {real_expected_rev_0}')\nprint(f'Classification by model probability: {classify_sample_revenue(real_expected_rev_1, real_expected_rev_0)}')\n\nExpected revenue class 1: -3.0\nExpected revenue class 0: -17.0\nClassification by model probability: 1\n\nReal expected revenue class 1: -4.5\nReal expected revenue class 0: -0.5\nClassification by model probability: 0\n\n\n\n\n\n\nAlgorithms not trained using a probabilistic framework\nThere are only a few models that produce calibrated probabilities. That is because in order to give calibrated probabilities, the model must be trained in a probabilistic framework, such as maximum likelihood estimation. The main example that can return probabilities already calibrated is a Logistic Regression. ANNs can also have better calibration of probabilities than other models.\nThe opposite can occur for methods such as in Bagging Estimators. For a Random Forest to predict a probability = 0, it means that all estimators in the forest predict 0. In a Random Forest this is harder to occur because of the feature subsample used to build each tree, meaning that there is a relative high variance between the trees in the forest.\nClass imbalance\nWhen there is a case of high class imbalance, such as in fraud prevention problems, the model will naturally predict higher probabilities for the majority class.\n\n\n\n\nFor the rest of this post, we’ll use the Credit Card Fraud Detection dataset available on Kaggle. Our goal will be to train a model to predict fraud and then use the same framework as before to classify each example as a fraud or not considering the expected profits and costs with each classification\n\ndf = pd.read_csv('creditcard.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n0.090794\n-0.551600\n-0.617801\n-0.991390\n-0.311169\n1.468177\n-0.470401\n0.207971\n0.025791\n0.403993\n0.251412\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n-0.166974\n1.612727\n1.065235\n0.489095\n-0.143772\n0.635558\n0.463917\n-0.114805\n-0.183361\n-0.145783\n-0.069083\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n0.207643\n0.624501\n0.066084\n0.717293\n-0.165946\n2.345865\n-2.890083\n1.109969\n-0.121359\n-2.261857\n0.524980\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n-0.054952\n-0.226487\n0.178228\n0.507757\n-0.287924\n-0.631418\n-1.059647\n-0.684093\n1.965775\n-1.232622\n-0.208038\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n0.753074\n-0.822843\n0.538196\n1.345852\n-1.119670\n0.175121\n-0.451449\n-0.237033\n-0.038195\n0.803487\n0.408542\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n\n\n\nAs expected for fraud prediction problems, we can see there is a high class imbalance between the classes:\n\ndf['Class'].value_counts(normalize = True)\n\n0    0.998273\n1    0.001727\nName: Class, dtype: float64\n\n\nLet’s perform an undersample of the majority class to get only 100.000 samples from class 0 and keep all samples from class 1:\n\nclass_0 = df.query('Class == 0').sample(100_000)\nclass_1 = df.query('Class == 1')\n\nfraud_data = pd.concat([class_0, class_1], axis = 0).reset_index(drop = True)\n\n\nfraud_data.Class.value_counts(normalize = True)\n\n0    0.995104\n1    0.004896\nName: Class, dtype: float64\n\n\nThere is still a very high class imbalance, but now it will be a bit easier to fit the model.\nNext, let’s split the data in a train and test sets and train an initial model:\n\nX = fraud_data.drop(['Class'], axis = 1)\ny = fraud_data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X_train, y_train);\n\n\nmodel_preds = model.predict(X_test)\n\nprint(classification_report(y_test, model_preds))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     25006\n           1       0.93      0.80      0.86       117\n\n    accuracy                           1.00     25123\n   macro avg       0.96      0.90      0.93     25123\nweighted avg       1.00      1.00      1.00     25123\n\n\n\nGreat! The model seems to present a nice performance on the test. Now, let’s move to the calibration of probabilities.\n\n\n\n\n\nTo better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn, this can be easily done with sklearn.calibration.calibration_curve. Even though scikit-learn has a built in class to plot the calibration curve, I personally prefer plotly as a library for visualization. With that in mind, we can use the following function to plot the calibration curve:\n\nfrom sklearn.calibration import calibration_curve\n\ndef plot_calibration_curve(models_probabilities, n_bins = 5, title = 'Calibration Curve'):\n\n    fig = sp.make_subplots(rows=1, cols=1)\n\n    for model_name, model_probs in list(models_probabilities.items()):\n        prob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=n_bins)\n\n        fig.add_trace(\n            go.Scatter(x=prob_pred, y=prob_true, mode='lines+markers', name=model_name),\n            row=1, col=1\n        )\n\n    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), name='Perfectly Calibrated'), row=1, col=1)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Mean Predicted Probability\",\n        yaxis_title=\"Fraction of Positives\",\n        showlegend=True, \n        autosize = False, \n        width = 800, height = 500\n    )\n    \n    fig.show()\n    \n\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Prepare input \nmodels_probabilities = {'Extra Trees':model_probs}\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 10, title = 'Extra Trees calibration curve')\n\n\n                                                \n\n\nThe number of bins can be configured to calculate the fraction of positives. If we have to many of them, each bin will have few points and it will be difficult to understand the calibration, since the graph will have a lot of noise. And the opposite will occur if we have too few of them. Here is an example of this difference:\n\nplot_calibration_curve(models_probabilities, n_bins = 20, title = 'Calibration Curve with 20 bins')\n\n\n                                                \n\n\n\nplot_calibration_curve(models_probabilities, n_bins = 3, title = 'Calibration Curve with 3 bins')\n\n\n                                                \n\n\nOverall, I find that somewhere between 5 and 10 is good place to select the number of bins\nLet’s also train a few more models so we can compare them:\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\n# LGBM, param grid optimized with optuna to maximize recall\nparam_grid_lgbm = {'n_estimators': 392,\n                    'bagging_freq':1,\n                    'learning_rate': 0.010990720231827398,\n                    'num_leaves': 14,\n                    'subsample': 0.8558243397218831,\n                    'colsample_bytree': 0.8486401096993541,\n                    'min_data_in_leaf': 53}\n                    \nlgbm = LGBMClassifier(verbose = -1, \n    class_weight = class_weight, \n    **param_grid_lgbm\n)\nlgbm.fit(X_train, y_train)\n\n# KNN, param grid optimized with optuna to maximize recall\nparam_grid_knn = {'n_neighbors': 5,\n                    'weights': 'uniform',\n                    'algorithm': 'kd_tree',\n                    'metric': 'manhattan'}\nknn = KNeighborsClassifier(**param_grid_knn)\nknn.fit(X_train, y_train)\n\n\n# Extra Trees, param grid optimized with optuna to maximize recall\nparam_grid_et = {\n  'n_estimators': 449,\n  'max_depth': 50,\n  'max_features': 'sqrt',\n  'min_samples_leaf': 68, \n  'class_weight':class_weight, \n  'bootstrap': True\n}\net = ExtraTreesClassifier(**param_grid_et)\net.fit(X_train, y_train);\n\n\n# Prepare input \nmodels_probabilities = {'Extra Trees'        : et.predict_proba(X_test)[:, 1], \n                        'Naive Bayes'        : nb.predict_proba(X_test)[:, 1], \n                        'LGBM'               : lgbm.predict_proba(X_test)[:, 1], \n                        'KNN'                : knn.predict_proba(X_test.values)[:, 1], \n                    }\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 5)\n\n\n                                                \n\n\n\n\n\nThe Brier Score simply calculates the mean squared difference between the model’s probabilities and the actual outcome. The score ranges from 0 to 1, and the lower the better\n\\[Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - p_i)^2\\]\nSo now let’s calculate the brier score with the original probabilities from the models:\n\nfrom sklearn.metrics import brier_score_loss\n\nfor model_name, prob in list(models_probabilities.items()):\n    print(f\"{model_name} brier score: {brier_score_loss(y_test, prob)}\")\n\nExtra Trees brier score: 0.04290825440034065\nNaive Bayes brier score: 0.008200641198818524\nLGBM brier score: 0.005798401913249227\nKNN brier score: 0.0036651673765075825\n\n\n\n\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, in which the process is repeated over k times.\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities, available as the parameter method = 'sigmoid' in the CalibratedClassifierCV class.\nThe other way is to an Isotonic Regression, that functions as a non-parametrical approach since it doesn’t make any assumptions about how the data is distributed. Using an isotonic regression is more powerful, but it also requires more training data. Scikit-learn’s documentation recommends to have at least 1000 samples in each fold. In our case we have over 75k samples for training, so that won’t be an issue. In this example we’ll use a isotonic regression through the parameter method = 'isotonic' in the CalibratedClassifierCV class.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nclf_list = [\n    (knn, \"KNN\"),\n    (nb, \"Naive Bayes\"),\n    (lgbm, \"LGBM\"),\n    (et, \"Extra Trees\"),\n]\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5, method = 'isotonic')\n    calibrated_clf.fit(X_train.values, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\nWe can plot again the new calibration curve:\n\nmodels_probabilities_calibrated = {model_name: model_calibrated.predict_proba(X_test.values)[:, 1] \\\n                                              for model_name, model_calibrated in calibrated_clfs.items()}\n\nplot_calibration_curve(models_probabilities_calibrated, n_bins = 5)\n\n\n                                                \n\n\n\n\n\n\n\nNow that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\n# Create a new dataframe with the calibrated probabilities\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_et = calibrated_clfs['Extra Trees'].predict_proba(X_test)[:, 1], \n    probs_knn = calibrated_clfs['KNN'].predict_proba(X_test.values)[:, 1],\n    y_true = y_test\n)\n\nWe can use the same functions as before to calculate the expected revenue for each class and then calculate what class the sample will be based on its revenue.\nAfter the predictions are made, we can access the real revenue by classifying the prediction as a TP/TN/FN/FP and calculating the costs/profits generated by a TP/TN/FN/FP classification. We can calculate this with the following functions:\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\nNow, we just need to calculate the revenues from the calibrated models:\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_et', 'probs_knn']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n    \n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n\n        # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n\n        # Gets the identification if the prediction is a TP/TN/FN/FP\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n\n        # Returns the real revenue by the prediction\n        real_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.real_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.real_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  219393.72\n\n probs_nb expected revenue:  186849.94\n\n probs_et expected revenue:  218477.48\n\n probs_knn expected revenue:  209727.42\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\n\n\n\n\n\n\n\n\nrevenues_not_calibrated\nrevenues_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n178397.34\n219393.72\n40996.38\n\n\nprobs_nb\n182901.17\n186849.94\n3948.77\n\n\nprobs_et\n219313.71\n218477.48\n-836.23\n\n\nprobs_knn\n203350.95\n209727.42\n6376.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nold_brier_score\nnew_brier_score\nbrier_score_diff\n\n\n\n\nprobs_lgbm\n0.0058\n0.00132\n-0.00448\n\n\nprobs_nb\n0.0082\n0.00343\n-0.00477\n\n\nprobs_et\n0.00107\n0.00151\n0.00044\n\n\nprobs_knn\n0.00367\n0.00351\n-0.00016\n\n\n\n\n\n\n\nIn general, we could improve the total revenue results with the calibration of probabilities. We can verify that the models with the highest difference in the brier score were also the ones with the highest increase in revenue. For models that had good calibration originally, such as the Extra Trees classifier, the process didn’t increase revenue.\nFinally, we can calculate what is the best model based on the total revenue generated by it and how much we could improve it with the calibration of probabilities:\n\nbest_model   = compare_revs.revenues_calibrated.idxmax()\n\nrevenue      = compare_revs.loc[best_model].revenues_calibrated.round(2)\nrevenue_diff = compare_revs.loc[best_model].revenue_diff.round(2)\n\nprint(f'''\nBest model probabilities are from {best_model}, with a total revenue of {revenue}. \nThe revenue increase after calibrating probabilities was a total of {revenue_diff}\n ''')\n\n\nBest model probabilities are from probs_lgbm, with a total revenue of 219393.72. \nThe revenue increase after calibrating probabilities was a total of 40996.38\n \n\n\n\n\n\nCalibrating probabilities can be useful in a lot a different scenarios when we are working with the real probabilities and not only caring for classifing examples as 1/0. I believe that the cost classification model can be very useful since its metrics are the real profits and costs involved in the model’s predictions, and for the business, in the end, that’s really what matters. Working with calibrating probabilities in this specific framework can have a real increase in these metrics.\nEven though there were a lot concepts here, we are only scratching the surface in these topics. I recently discovered this framework of “Cost-sensitive learning” and found there is a range of different techniques in it. For anyone interested in this topic, I recommend the paper The Foundations of Cost-Sensitive Learning, by Charles Elkan.\nWhen it comes to the calibration of probabilities, there is also many more techniques that could be applied to improve calibration. For a more technical analysis on the subject, I recommend the chapter of Probability Calibration from the book Imbalanced Binary Classification: A Survey with code, where the authors explore different calibration techniques and focus on how to apply calibration in imbalanced problems, such as the one we worked with here."
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "href": "posts/probability-calibration/prob_calibration.html#calibrating-a-classifier",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "To better understand our model calibration, we can use a calibration curve (or realiability diagram) to understand how our probabilities are being distributed. This type of diagram plots the frequency of the positive label in the y-axis and the predicted probabilities on the x-axis.\nThe way that scikit-learn actually does this is to bin each prediction from the model, such that in the x-axis we’ll have the average predict probability for each bin, and in the y-axis we’ll have the fraction of positives in that same bin\nUsing scikit-learn, this can be easily done with sklearn.calibration.calibration_curve. Even though scikit-learn has a built in class to plot the calibration curve, I personally prefer plotly as a library for visualization. With that in mind, we can use the following function to plot the calibration curve:\n\nfrom sklearn.calibration import calibration_curve\n\ndef plot_calibration_curve(models_probabilities, n_bins = 5, title = 'Calibration Curve'):\n\n    fig = sp.make_subplots(rows=1, cols=1)\n\n    for model_name, model_probs in list(models_probabilities.items()):\n        prob_true, prob_pred = calibration_curve(y_test, model_probs, n_bins=n_bins)\n\n        fig.add_trace(\n            go.Scatter(x=prob_pred, y=prob_true, mode='lines+markers', name=model_name),\n            row=1, col=1\n        )\n\n    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash'), name='Perfectly Calibrated'), row=1, col=1)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Mean Predicted Probability\",\n        yaxis_title=\"Fraction of Positives\",\n        showlegend=True, \n        autosize = False, \n        width = 800, height = 500\n    )\n    \n    fig.show()\n    \n\n\n# Get probabilities from model \nmodel_probs = model.predict_proba(X_test)[:, 1]\n\n# Prepare input \nmodels_probabilities = {'Extra Trees':model_probs}\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 10, title = 'Extra Trees calibration curve')\n\n\n                                                \n\n\nThe number of bins can be configured to calculate the fraction of positives. If we have to many of them, each bin will have few points and it will be difficult to understand the calibration, since the graph will have a lot of noise. And the opposite will occur if we have too few of them. Here is an example of this difference:\n\nplot_calibration_curve(models_probabilities, n_bins = 20, title = 'Calibration Curve with 20 bins')\n\n\n                                                \n\n\n\nplot_calibration_curve(models_probabilities, n_bins = 3, title = 'Calibration Curve with 3 bins')\n\n\n                                                \n\n\nOverall, I find that somewhere between 5 and 10 is good place to select the number of bins\nLet’s also train a few more models so we can compare them:\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\n# LGBM, param grid optimized with optuna to maximize recall\nparam_grid_lgbm = {'n_estimators': 392,\n                    'bagging_freq':1,\n                    'learning_rate': 0.010990720231827398,\n                    'num_leaves': 14,\n                    'subsample': 0.8558243397218831,\n                    'colsample_bytree': 0.8486401096993541,\n                    'min_data_in_leaf': 53}\n                    \nlgbm = LGBMClassifier(verbose = -1, \n    class_weight = class_weight, \n    **param_grid_lgbm\n)\nlgbm.fit(X_train, y_train)\n\n# KNN, param grid optimized with optuna to maximize recall\nparam_grid_knn = {'n_neighbors': 5,\n                    'weights': 'uniform',\n                    'algorithm': 'kd_tree',\n                    'metric': 'manhattan'}\nknn = KNeighborsClassifier(**param_grid_knn)\nknn.fit(X_train, y_train)\n\n\n# Extra Trees, param grid optimized with optuna to maximize recall\nparam_grid_et = {\n  'n_estimators': 449,\n  'max_depth': 50,\n  'max_features': 'sqrt',\n  'min_samples_leaf': 68, \n  'class_weight':class_weight, \n  'bootstrap': True\n}\net = ExtraTreesClassifier(**param_grid_et)\net.fit(X_train, y_train);\n\n\n# Prepare input \nmodels_probabilities = {'Extra Trees'        : et.predict_proba(X_test)[:, 1], \n                        'Naive Bayes'        : nb.predict_proba(X_test)[:, 1], \n                        'LGBM'               : lgbm.predict_proba(X_test)[:, 1], \n                        'KNN'                : knn.predict_proba(X_test.values)[:, 1], \n                    }\n\n# Plot curve\nplot_calibration_curve(models_probabilities, n_bins = 5)\n\n\n                                                \n\n\n\n\n\nThe Brier Score simply calculates the mean squared difference between the model’s probabilities and the actual outcome. The score ranges from 0 to 1, and the lower the better\n\\[Brier Score = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - p_i)^2\\]\nSo now let’s calculate the brier score with the original probabilities from the models:\n\nfrom sklearn.metrics import brier_score_loss\n\nfor model_name, prob in list(models_probabilities.items()):\n    print(f\"{model_name} brier score: {brier_score_loss(y_test, prob)}\")\n\nExtra Trees brier score: 0.04290825440034065\nNaive Bayes brier score: 0.008200641198818524\nLGBM brier score: 0.005798401913249227\nKNN brier score: 0.0036651673765075825\n\n\n\n\nScikit-learn provides a module for calibrating probabilities with the CalibratedClassifierCV class. This method uses cross-validation to fit the model in a training set and then calibrate the probabilities in a test set, in which the process is repeated over k times.\nThere is more than one way to calibrate probabilities. The simplest one is to apply a logistic regression model to transform probabilities, available as the parameter method = 'sigmoid' in the CalibratedClassifierCV class.\nThe other way is to an Isotonic Regression, that functions as a non-parametrical approach since it doesn’t make any assumptions about how the data is distributed. Using an isotonic regression is more powerful, but it also requires more training data. Scikit-learn’s documentation recommends to have at least 1000 samples in each fold. In our case we have over 75k samples for training, so that won’t be an issue. In this example we’ll use a isotonic regression through the parameter method = 'isotonic' in the CalibratedClassifierCV class.\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nclf_list = [\n    (knn, \"KNN\"),\n    (nb, \"Naive Bayes\"),\n    (lgbm, \"LGBM\"),\n    (et, \"Extra Trees\"),\n]\n\ncalibrated_clfs = {}\n\nfor base_clf, clf_name in clf_list: \n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=5, method = 'isotonic')\n    calibrated_clf.fit(X_train.values, y_train)\n    calibrated_clfs[clf_name] = calibrated_clf\n\nWe can plot again the new calibration curve:\n\nmodels_probabilities_calibrated = {model_name: model_calibrated.predict_proba(X_test.values)[:, 1] \\\n                                              for model_name, model_calibrated in calibrated_clfs.items()}\n\nplot_calibration_curve(models_probabilities_calibrated, n_bins = 5)"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "href": "posts/probability-calibration/prob_calibration.html#cost-classification-model",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "Now that we have our probabilities calibrated, we can return to the original problem: let’s write a code that classifies a transaction as a fraud or not taking into account the costs and profits involved with each prediction\n\n# Create a new dataframe with the calibrated probabilities\ndf_test_models_calib = X_test.assign(\n    probs_lgbm = calibrated_clfs['LGBM'].predict_proba(X_test)[:, 1], \n    probs_nb = calibrated_clfs['Naive Bayes'].predict_proba(X_test)[:, 1], \n    probs_et = calibrated_clfs['Extra Trees'].predict_proba(X_test)[:, 1], \n    probs_knn = calibrated_clfs['KNN'].predict_proba(X_test.values)[:, 1],\n    y_true = y_test\n)\n\nWe can use the same functions as before to calculate the expected revenue for each class and then calculate what class the sample will be based on its revenue.\nAfter the predictions are made, we can access the real revenue by classifying the prediction as a TP/TN/FN/FP and calculating the costs/profits generated by a TP/TN/FN/FP classification. We can calculate this with the following functions:\n\ndef classify_sample(example, y_true): \n    if example == 1 and y_true == 1: return 'tp'\n    elif example == 1 and y_true == 0: return 'fp'\n    elif example == 0 and y_true == 0: return 'tn'\n    elif example == 0 and y_true == 1: return 'fn'\n\ndef calculate_matrix_revenue(cfmatrix_class, amount): \n    if cfmatrix_class == 'tp': return  0 \n    elif cfmatrix_class == 'fp': return -(amount * 0.1)\n    elif cfmatrix_class == 'fn': return -amount \n    elif cfmatrix_class == 'tn': return (amount * 0.1) \n\nNow, we just need to calculate the revenues from the calibrated models:\n\nrevenues_calibrated = {}\n\nfor p in ['probs_lgbm', 'probs_nb', 'probs_et', 'probs_knn']:\n    expected_rev_1, expected_rev_0 = get_example_value_dependent(df_test_models_calib[p], df_test_models_calib['Amount'])\n    \n    df_rev = df_test_models_calib.assign(\n        expected_rev_1 = expected_rev_1,\n        expected_rev_0 = expected_rev_0,\n\n        # Calculates prediction if expected_rev_1 &gt; expected_rev_0\n        pred_fraud_cost_dep = lambda d: d.apply(lambda x: 1 if x['expected_rev_1'] &gt; x['expected_rev_0'] else 0, axis = 1),\n\n        # Gets the identification if the prediction is a TP/TN/FN/FP\n        cfmatrix_class = lambda d: d.apply(lambda x: classify_sample(x['pred_fraud_cost_dep'], x['y_true']),  axis = 1), \n\n        # Returns the real revenue by the prediction\n        real_revenue = lambda d: d.apply(lambda x: calculate_matrix_revenue(x['cfmatrix_class'], x['Amount']),  axis = 1)\n    )\n\n    revenues_calibrated[p] = round(df_rev.real_revenue.sum(), 2)\n    print(f'\\n {p} expected revenue: ', round(df_rev.real_revenue.sum(), 2))\n\n\n probs_lgbm expected revenue:  219393.72\n\n probs_nb expected revenue:  186849.94\n\n probs_et expected revenue:  218477.48\n\n probs_knn expected revenue:  209727.42\n\n\nLet’s also take a look at how different these predictions would be had we not calibrated our probabilities before:\n\n\n\n\n\n\n\n\n\nrevenues_not_calibrated\nrevenues_calibrated\nrevenue_diff\n\n\n\n\nprobs_lgbm\n178397.34\n219393.72\n40996.38\n\n\nprobs_nb\n182901.17\n186849.94\n3948.77\n\n\nprobs_et\n219313.71\n218477.48\n-836.23\n\n\nprobs_knn\n203350.95\n209727.42\n6376.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nold_brier_score\nnew_brier_score\nbrier_score_diff\n\n\n\n\nprobs_lgbm\n0.0058\n0.00132\n-0.00448\n\n\nprobs_nb\n0.0082\n0.00343\n-0.00477\n\n\nprobs_et\n0.00107\n0.00151\n0.00044\n\n\nprobs_knn\n0.00367\n0.00351\n-0.00016\n\n\n\n\n\n\n\nIn general, we could improve the total revenue results with the calibration of probabilities. We can verify that the models with the highest difference in the brier score were also the ones with the highest increase in revenue. For models that had good calibration originally, such as the Extra Trees classifier, the process didn’t increase revenue.\nFinally, we can calculate what is the best model based on the total revenue generated by it and how much we could improve it with the calibration of probabilities:\n\nbest_model   = compare_revs.revenues_calibrated.idxmax()\n\nrevenue      = compare_revs.loc[best_model].revenues_calibrated.round(2)\nrevenue_diff = compare_revs.loc[best_model].revenue_diff.round(2)\n\nprint(f'''\nBest model probabilities are from {best_model}, with a total revenue of {revenue}. \nThe revenue increase after calibrating probabilities was a total of {revenue_diff}\n ''')\n\n\nBest model probabilities are from probs_lgbm, with a total revenue of 219393.72. \nThe revenue increase after calibrating probabilities was a total of 40996.38"
  },
  {
    "objectID": "posts/probability-calibration/prob_calibration.html#final-thoughts",
    "href": "posts/probability-calibration/prob_calibration.html#final-thoughts",
    "title": "Probability Calibration & Cost classification models",
    "section": "",
    "text": "Calibrating probabilities can be useful in a lot a different scenarios when we are working with the real probabilities and not only caring for classifing examples as 1/0. I believe that the cost classification model can be very useful since its metrics are the real profits and costs involved in the model’s predictions, and for the business, in the end, that’s really what matters. Working with calibrating probabilities in this specific framework can have a real increase in these metrics.\nEven though there were a lot concepts here, we are only scratching the surface in these topics. I recently discovered this framework of “Cost-sensitive learning” and found there is a range of different techniques in it. For anyone interested in this topic, I recommend the paper The Foundations of Cost-Sensitive Learning, by Charles Elkan.\nWhen it comes to the calibration of probabilities, there is also many more techniques that could be applied to improve calibration. For a more technical analysis on the subject, I recommend the chapter of Probability Calibration from the book Imbalanced Binary Classification: A Survey with code, where the authors explore different calibration techniques and focus on how to apply calibration in imbalanced problems, such as the one we worked with here."
  },
  {
    "objectID": "course-notes/econometrics-ml/first-lecture/index.html",
    "href": "course-notes/econometrics-ml/first-lecture/index.html",
    "title": "First lecture",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Probability Calibration & Cost classification models\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJulia Pocciotti\n\n\n\n\n\n\nNo matching items"
  }
]